---
layout: single
title: "[Deep Learning 2] 활성화 함수와 3층 신경망(MNIST 활용)"
categories: DeepLearning
tag: [DL]
toc: true # table of contents
author_profile: false # 각 콘텐츠에서 프로필 여부
sidebar: # 사이드바 설정
    nav: "docs"
search: true # 검색 여부 설정
---

<style>
    .r {
        color: red;
    }
</style>

## MNIST

자꾸 코랩에서 Error: 403이 뜨길래 별의 별 짓을 다 해봤다. 직접 케라스에서 데이터셋을 다운 받기도 해보고, 스택 오버플로우에 있는 코드도 복붙해보고. 로컬에다가 아나콘다랑 가상머신을 돌려서 처리해보려고도 하고...

그런데 오류가 발생한 지점은 애석하게도 MNIST 데이터셋을 갖고 있는 url이었다.. 이 책이 도서관에서 빌려온 거라 7년 전 초판3쇄 발행본이다보니, 현재의 바뀐 링크가 아니어서 403이 떴던 것..

진짜 대여섯 시간동안 고민했는데 이렇게 단순한 문제였다니.. 앞으론 책 버전에 유의하며 코딩해야겠다!

### Colab에서 MNIST 작업하기

이하의 설명은 케라스 등의 라이브러리에서 데이터셋을 다운받는 것이 아닌, 책의 깃허브 링크에 존재하는 mnist.py 및 피클(pkl) 파일을 통해 진행한 것이다.

일단 책의 mnist.py 코드와 sample_weight.pkl 파일을 내가 작업 중인 구글 드라이브의 폴더로 갖고 온다.

또한 이전 포스팅에서 작성한 sigmoid()와 softmax() 함수를 가져온다. 전체적인 밑작업 코드는 다음과 같다.

```python
import numpy as np
import sys, os # 구글 드라이브와 연동용
import pickle # 필요한지는 모르겠는데 일단 써놓음(안전빵)
import requests # 피클이 인식이 안돼서 부득이하게 추가
sys.path.append('/content/drive/<여기에 주소 입력>') # 내가 저장한 구글 드라이브 위치

from PIL import Image # MNIST 이미지 확인용
from matplotlib.pyplot import imshow # 원래 이걸로 확인해야하는데 안되더라?
from mnist import load_mnist # 책의 깃헙 링크에서 가져온 mnist.py 임포트

def sigmoid(x): # 활성화 함수 1
    return 1 / (1 + np.exp(-x))

def softmax(a): # 활성화 함수 2
    c = np.max(a)
    exp_a = np.exp(a - c)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a

    return y
```

먼저 load_mnist()로 MNIST 데이터셋을 가져온다.

flatten은 3차원(1*28*28) 원본 데이터를 1차원(784)으로 압축시키는 여부, normalize는 전처리로 정규화(0~1 사이로 만듦)할 건지 여부, one_hot_label은 원-핫 인코딩을 할 지 여부이다.

```python
(x_train, t_train), (x_test, t_test) = load_mnist(flatten=True, normalize=False)

print(x_train.shape) # (60000, 784)
print(t_train.shape) # (60000,)
print(x_test.shape) # (10000, 784)
print(x_test.shape) # (10000,)
```

이후 깔쌈하게 이미지 하나를 봐보자. MNIST가 정확히 들어왔음을 육안으로 확인할 수 있다.

```python
def img_show(img):
    pil_img = Image.fromarray(np.uint8(img))
    pil_img.show()

img = x_train[0]
label = t_train[0]
print(label) # 5

print(img.shape) # (784,)
img = img.reshape(28, 28) # 1차원 압축본을 2차원으로 변환
print(img.shape) # (28, 28)

img_show(img) # 원래 이 코드로 돼야 하는데 안돼서
imshow(img) # 맷플롯립의 imshow()로 시각화 대체
```

<img src="https://user-images.githubusercontent.com/109167/28967532-d00cfb86-791b-11e7-8677-e13b58de99cf.png">

요렇게 뜨면 잘 되고 있는거다

이제 피클(pickle)을 통해 빠르게 MNIST 객체를 불러올 건데, 이상하게 이게 또 안됐다. 그래서 부득이하게 링크로부터 파일을 받아봐 저장 후, 다시 여는 방식으로 사용했다.

```python
def get_data():
    (x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, flatten=True, one_hot_label=False)
    return x_test, t_test

# 수정한 부분: pickle 파일을 깃헙에서 다운받은 후 다시 엶
def init_network():
    url = "https://github.com/WegraLee/deep-learning-from-scratch/raw/master/ch03/sample_weight.pkl"
    response = requests.get(url) # 깃헙에서 다운받은 후
    with open("sample_weight.pkl", 'wb') as file: # sample_weight.pkl로 저장 후
        file.write(response.content)
    with open("sample_weight.pkl", 'rb') as f: # 다시 엶
        network = pickle.load(f)
    return network


def predict(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']

    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    y = softmax(a3)

    return y
```

이제 코드를 돌려 결괏값을 얻어보자.

```python
x, t = get_data()
network = init_network()
accuracy_cnt = 0
for i in range(len(x)):
    y = predict(network, x[i])
    p= np.argmax(y) # 확률이 가장 높은 원소의 인덱스를 얻는다.
    if p == t[i]: # 인덱스가 일치하면(정확히 예측했으면)
        accuracy_cnt += 1 # 카운트 +1

print("Accuracy:" + str(float(accuracy_cnt) / len(x))) # 0.9352
```

### 배치 처리(일괄 처리)로 코드 개선

우리가 다루는 데이터는 array, 즉 어쨌든 행렬이기 때문에 굳이 데이터를 하나씩 입력할 필요는 없다.

행렬의 곱의 특성상 맨 앞 행렬의 '행' 성분은 보존되기 때문에, 배치(batch)로 처리하면 계산 과정이 줄어 더 빠르다.

```python
x, t = get_data()
network = init_network()

batch_size = 100
accuracy_cnt = 0

for i in range(0, len(x), batch_size):
    x_batch = x[i:i+batch_size] # batch_size만큼의 개수를 입력
    y_batch = predict(network, x_batch)
    p = np.argmax(y_batch, axis=1) # 확률이 가장 높은 원소의 인덱스들을 얻는다.
    accuracy_cnt += np.sum(p == t[i:i+batch_size]) # 정확히 예측한만큼 카운트

print("Accuracy:" + str(float(accuracy_cnt) / len(x))) # 0.9352
```




