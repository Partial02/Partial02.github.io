---
layout: single
title: "[확률 및 랜덤 과정 2주차] 조건부 확률"
categories: Probability
tag: [MATH, COSE382]
toc: true # table of contents
author_profile: false # 각 콘텐츠에서 프로필 여부
sidebar: # 사이드바 설정
    nav: "docs"
search: true # 검색 여부 설정
---
<head>
    <!-- Latex -->
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<style>
    th, td {
        text-align: center;
    }
    .r {
        color: red;
    }
</style>

이번 주부터 연습 문제가 나간다는데, 어느 정도일지? 수리통계학보단 쉽겠지?

## Sep 9(Tue)

### 조건부 확률(Conditional Probability)이란?

세상은 독립적으로 이뤄지지 않곤 한다. 무언가 인과관계가 있어 먼저 사건 A가 발생하고, 그에 따른 결과로 사건 B가 발생하곤 한다. 따라서 조건부 확률은 논리적이고(logical) 일관되게(coherent) 세상에 먼저 일어난 일들을 통해 세상을 이해하게 돕는다(incorporate evidence into our understanding of the world).

조건부 확률의 정의는 너무 잘 알려져있다.

$$P(A|B):=\frac{P(A\cap B)}{P(B)}\;\text{for}\;P(B)>0$$

여기서 A와 같은 단일 확률을 사전 확률(prior probability), 조건부 확률과 같은 형태를 사후 확률(posterior probability), 사후 확률의 선험적 사건이 되는 확률 P(B)를 evidence probability라 한다.

이 조건부 확률은 

$$P(A\cap B)=P(B)P(A|B)=P(A)P(B|A)$$

처럼 교집합에 대한 곱셈의 꼴로 변형할 수 있고, 

$$P(A,A_2,\cdots ,A_n)=P(A_1)P(A_2|A_1)\cdots P(A_n|A_1,\cdots ,A_{n-1})$$

이렇게 시계열과 같이 recursive하게 이해할 수도 있다(이 경우 텍스트 생성에 쓰인다)

### 베이즈 정리(Bayes' Rule)

이 조건부 확률을 조금 튼 것이 <strong class="r">베이즈 정리</strong>인데

$$P(A|B)=\frac{P(B|A)P(A)}{P(B)}$$

다음과 같이 사건 B에 대한 사건 A의 조건부 확률을, 사건 A에 대한 사건 B의 조건부 확률로 구할 수 있다는 특징이 있다. 즉 사건이 일어나지 않더라도, 그 선험적 확률들을 가지고 유추가 가능하다는 것

여기서 P(B/A)와 같이 조건부 확률 중 순서를 바꾼 확률을 전이 확률(transition probability)이라고 한다. 따라서

<strong class="r">$$(posterior)=\frac{(transition)\times (prior)}{(evidence)}$$</strong>

이렇게 이해할 수 있다. 사전에 발생한 evidence와 기본적 확률인 prior, 그리고 순서를 바꾼 transition이 있다면, 사후 확률인 posterior가 계산 가능하다는 의미로 이해하면 된다.

#### 전확률 정리(Law of Total Probability; LOTP)

이 베이즈 정리를 조금 확장해보자. 고등학교 교육 과정에서 빠진 분할(partition)이라는 개념이 있다. 한 집합을 여러 개의 서로소 집합으로 나누어 (기계학습의 표현을 빌리자면) Mutually exclusive and exhaustive하게 쪼갠 것을 분할이라고 한다. 이 분할들을 합친 원래 사건을 표본공간 S라 하면

$$S=\dot\bigcup_{i=1}^nA_i\text{, then }P(B)=\sum P(B|A_i)P(A_i)$$

이를 전확률 정리, 혹은 <strong class="r">LOTP</strong>라 한다. 확률 공리 및 결합 법칙으로 증명도 가능하다. 사건 B에 대해서 결합 법칙까지 적용하면

$$B=B\cap S=B\cap(\dot\bigcup_{k=1}^nA_k)=\dot\bigcup_{k=1}^n(B\cap A_k)$$

양변에 확률을 취하면 확률 공리 2번(시그마 가법성)과 LOTP에 의해

$$P(B)=\sum_{k=1}^nP(B\cap A_k)=\sum_{k=1}^nP(B|A_k)P(A_k)\;\blacksquare$$

알고리즘적으로 이해하자면 분할정복(divide & conqure)에 해당하겠다

#### Ex 2.3.9

간단하게 에제 하나만 봐보자. 전체 인구의 1%에게만 걸리는 질병이 있고, 이 질병에 대한 검진은 95% 정확하다. 정확히는 '확진인 사건' D와 '검사 결과 양성인 사건' T에 대하여 

$$P(T|D)=P(T^c|D^c)=0.95$$

인 것이다. 그렇다면 내가 양성이 떴다고 했을 때, 실제 양성일 확률은 얼마인가?

$$\begin{align} P(D|T)&=\frac{P(T|D)P(D)}{P(T)}=\frac{P(T|D)P(D)}{P(T|D)P(D)P(T|D^c)P(D^c)}\\&=\frac{0.95\cdot0.01}{0.95\cdot0.01+0.05\cdot0.99}\approx 0.16\end{align}$$

16%밖에 안된다. 죽을 걱정은 던 것 같다! 그런데 2번 연속 키트에서 양성이 떴다면 어떻게 될까? 이것도 조건부 확률로 풀 수 있을까?


## Sep 11(Thu)

### 조건부 확률도 확률이다

영사건이 아닌 사건 E에 대해서 $$P(\cdot |E)$$라는 함수를 생각해보자. 여기서 분모의 E는 고정된 사건이고, 분자의 점은 argument, 즉 변수이다. 이 함수는 확률인가? 그렇다 확률이다. 왜냐? 콜모고로프의 확률 공리를 만족하니까

$$\text{(Axiom 1: Unit measure) }P(S|E)=1$$

$$\text{(Axiom 2: Sigma-additivity) }P(\dot\bigcup_{i=1}^n A_i|E)=\sum_{i=1}^nP(A_i|E)$$

따라서 우리는 이러한 정리들도 얻을 수 있다.

$$\text{(성질 1: 조건부 확률의 여사건) }P(A^c|E)=1-P(A|E)$$

$$\text{(성질 2: 조건부 확률의 영사건) }P(\phi|E)=0$$

$$\text{(성질 3: 조건부 확률의 합사건) }P(A\cup B|E)=P(A|E)+P(B|E)-P(A\cap B|E)$$

따라서 <strong class="r">조건부 확률은 확률이다. 그러나 $$P(E|\cdot)$$는 확률이 아니다.</strong> 그 이유는

$$P(E|\phi) : undefined\text{ (영사건이 argument에 들어갈 수 없다)}$$

$$P(E|S)=P(E)\neq 1\text{ (evidence가 S임에도 확률이 1이 되지 않는다)}$$

다만 확률이 아닐 뿐 함수인 것은 맞다. 따라서 $$P(E|\cdot)$$와 같이 분모에 인자가 있는 조건부 함수를 <strong class="r">우도 함수(likelihood function)</strong>라고 한다. 앞으로 자주 보게 될 친구이니 기억해 두자.

#### Extra Conditioning

그렇다면 조건부가 두 개 이상인 경우에도 베이즈 정리가 성립할까? 답은 Yes! 이다. 

$$\begin{align} P(A|B,E)&=\frac{P(A\cap B\cap E)}{P(B\cap E)}=\frac{P(B\cap A\cap E)}{P(B\cap E)}\\&=\frac{P(B\cap A\cap E)/P(E)}{P(B\cap E)/P(E)}\\&=\frac{\frac{P(B\cap A\cap E)}{P(A\cap E)}\times\frac{P(A\cap E)}{P(E)}}{\frac{P(B\cap E)}{P(E)}}\\&=\frac{P(B|A,E)P(A|E)}{P(B|E)} \end{align}$$

여기서 B와 E는 둘 다 evidence이므로 exchangable하다. 즉 <strong class="r">Bayes' rule with extra conditioning</strong>(Th 2.4.2)는

$$P(A|B,\textcolor{cyan}E)=\frac{P(B|A,\textcolor{cyan}E)P(A\textcolor{cyan}{|E})}{P(B\textcolor{cyan}{|E})}=\frac{P(E|A,\textcolor{blue}B)P(A\textcolor{blue}{|B})}{P(E\textcolor{blue}{|B})}$$

이를 원래의 베이즈 정리와 비교해보면

$$P(A|\textcolor{red}B)=\frac{P(B|\textcolor{red}A)P(\textcolor{red}A)}{P(\textcolor{red}B)}$$

보이는가? 추가된 <strong class="r">사건 E가 각 확률의 마지막에 추가</strong>되었다. 다만 교집합으로 묶였냐, evidence로 묶였냐의 차이이다. 이건 비교하여 잘 기억해 놓자. 확률이 posterior였으면 컴마로, 그냥 prior였으면 bar로 추가하면 되는 것이니 외우긴 쉬울 거다.

또한 LOTP도 조건부가 2개 이상인 경우에 성립한다.

$$P(B\textcolor{cyan}{|E})=\sum_{i=1}^nP(B|A_i,\textcolor{cyan}E)P(A_i\textcolor{cyan}{|E})$$

이것도 원래의 LOTP에서 위와 같은 방식으로 추가된 것이다. 비교하여 기억해두자.

$$P(B)=\sum_{i=1}^nP(B|\textcolor{red}{A_i})P(\textcolor{red}{A_i})$$

### 확률적 독립의 여러 성질

독립사건의 정의에 대해서는 너무나 잘 아니 패스.

독립사건 A, B에 대하여 $$P(A|B)=P(A),\;P(B|A)=P(B)$$가 성립한다는 것은 벤다이어그램 상에서 <strong class="r">B 영역 중 (A, B)가 차지하는 것과, 표본공간에서 A가 차지하는 영역의 비율이 같다</strong>는 것을 의미한다. 또한 vice versa.

또한 배타적 사건이거나 동일 사건인 경우는 확률적으로 종속(dependent)임도 알고 있다. 또한 하나 중요하게 기억해둬야할 것은 A와 B가 독립이면 <strong class="r">A의 여사건과 B, A와 B의 여사건, A의 여사건과 B의 여사건이 모두 독립</strong>이라는 것이다. 수식으로 증명하자면

$$P(B^c|A)=1-P(B|A)=1-P(B)=P(B^c)$$

$$\therefore \text{if }A\bot B\Rightarrow A\bot B^c,A^c\bot B,A^c\bot B^c$$

또한 독립사건끼리의 교집합의 확률은 각 사건을 모두 연쇄적으로 곱한 것과 같다. 뭐 이건 당연한거고

#### 조건부 독립(Conditional Independence)

조건부 독립이라는 것도 존재한다. 다음과 같을 때 <strong class="r">사건 A와 B는 사건 E에 대해 조건부 독립</strong>이다.

$$P(A\cap B|E)=P(A|E)P(B|E)$$

다만 다음과 같은 3가지 케이스를 유의해야한다.

<div class="notice--warning">
<h4>조건부 확률이 imply하지 않는 것</h4>
<ol>
<li>쌍별로 독립(pairwise independence)이라고 해서 세 사건이 항상 독립인 것은 아니다</li>
<li>조건부 독립(conditional independence)이라고 해서 두 사건이 서로 독립인 것은 아니다</li>
<li>역으로, 독립이라고 해서 두 사건이 조건부 독립인 것도 아니다.</li>
</ol></div>

관련한 예제들은 시험기간 때 다시 다뤄보도록 하자.

#### 몬티홀 문제(Monty Hall, Ex 2.7.1)

문제 상황은 너무 유명하니 LOTP를 사용한 설명만을 작성한다. C_i는 i번째 문 뒤에 차가 있는지에 대한 확률, state는 stay(안 바꿈) 혹은 switch(바꿈)이고 get car는 결과적으로 차를 얻는 사건을 의미한다. 

차가 각 문 뒤에 있을 확률은 동등하게 1/3이고, 참가한 사람이 최초에 1번 문을 골랐다고 해보자. LOTP에 의해

$$P(\text{get car}|\text{state})=\sum_{i=1}^3P(\text{get car}|C_i,\text{state})\cdot P(C_i|\text{state})$$

이므로 문을 바꾸지 않을 때(stay)는

$$P(\text{get car}|\text{stay})=1\cdot\frac{1}{3}+0\cdot\frac{1}{3}+0\cdot\frac{1}{3}=\frac{1}{3}$$

이고 문을 바꿀 때(switch)는

$$P(\text{get car}|\text{switch})=0\cdot\frac{1}{3}+1\cdot\frac{1}{3}+1\cdot\frac{1}{3}=\frac{2}{3}$$

이므로 <strong class="r">문을 바꾸는 것이 유리</strong>하다!

#### Branching Process(Ex 2.7.2)

문제 상황은 다음과 같다. '보보'라는 이름의 아메바 한 마리는 매 분마다 3가지 행동 중 하나를 한다. 죽거나(die), 아메바 2개로 증식하거나(split into two amoebas), 혹은 상태를 유지한다(stay the same). 보보의 종족들은 시간이 무수히 흘렀을 때 어떻게 될까?

최초 상태로부터 bobo의 군집이 모두 절멸할 확률을 P(D)라 했을 때 1분 뒤 보보의 마릿수가 i개라면, 절멸할 확률 P(D)의 조건부 확률은 다음과 같다.

$$P(D|B_0)=1\text{ / }P(D|B_1)=P(D)\text{ / }P(D|B_2)=P(D)^2$$

여기서 LOTP에 의해

$$\begin{align} P(D)&=P(D|B_0)\cdot\frac{1}{3}+P(D|B_1)\cdot\frac{1}{3}+P(D|B_2)\cdot\frac{1}{3}\\&=\frac{1}{3}+\frac{1}{3}P(D)+\frac{1}{3}P(D)^2 \end{align}$$

인데 양변을 정리하여 이차방정식을 풀면 <strong class="r">$$P(D)=1$$</strong>이다. <strong class="r">보보 일족은 결국 절멸한다..</strong>


출처: 확률및랜덤과정(COSE382) ㅈㅇㅈ 교수님
