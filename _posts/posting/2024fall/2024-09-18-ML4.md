---
layout: single
title: "[기계학습 3주차A] 베이즈 결정이론 & 최대우도추정"
categories: MachineLearning
tag: [ML, COSE362]
toc: true # table of contents
author_profile: false # 각 콘텐츠에서 프로필 여부
sidebar: # 사이드바 설정
    nav: "docs"
search: true # 검색 여부 설정
---
<head>
    <!-- Latex -->
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<style>
    th, td {
        text-align: center;
    }
    .r {
        color: red;
    }
</style>

앞서 베이지안 분류(Bayesian Classification)의 결론은 Posterior가 큰 쪽으로 분류하면 된다는 것이었다. 그런데 이게 유일한 기준이 될 수 있을까? 정말로 <strong class="r">Posterior가 큰 쪽으로 하는 것만이 유리할까?</strong>

## 베이즈 결정이론(Bayesian Decision Rules)

분류에 있어 위양성과 위음성의 에러가 있지만, 이 둘의 경중이 다를 수 있다. 가령 병에 안 걸린 사람에게 걸렸다고 진단하는 것은 큰 오류겠지만, 병에 걸린 사람에게 안 걸렸다고 오진하는 것은 치명적인 에러가 될 수 있다. 따라서 에러의 개수만을 작게 하는 것보다도, <strong class="r">risk가 가장 작게</strong> 하는 것이 중요하다.

입력 x를 클래스 $$c_i$$로 분류하는 action을 $$\alpha_i$$라고 하고, 실제 x는 클래스 k에 속한다고 하자. 이때 클래스 k에 속하는 입력 x를 클래스 i로 분류하였을 때 발생하는 비용(loss;cost)을 $$\lambda_{ik}$$라고 두면 risk의 평균(=기댓값)을 R이라 하면

$$R(\alpha_i|x)\equiv \sum_k\lambda_{ik}P(c_k|x)$$

즉 입력을 k클래스로 분류할 확률과 그에 따른 loss를 곱하여 다 더한 것이 평균 risk가 된다. 이때

$$\text{Choose }\alpha_i\text{ if }i=arg \min_{i'} R(\alpha_i|x)$$

risk가 최소가 되는 인덱스 i로 클래스를 판단하는 방법을 <strong class="r">Bayesian Decision Rule</strong>(베이즈 결정 규칙; Bayes Estimator)이라 한다.

### 베이즈 결정이론: 이진분류의 경우

베이지안 분류와 베이즈 결정이론을 합쳐서 생각하면

![binary_decision]({{site.url}}/images/MacLea/bayesian_decision_binary.png)

차근차근 이해해보자. 우리는 리스크가 작은 쪽으로 클래스를 분류할 것이다(1번째 줄). 이 리스크는 '1을 1로 판단+2를 1로 판단' 과 '1을 2로 판단+2를 2로 판단'으로 sigma를 풀어서 생각할 수 있다(2번째 줄).

양변을 확률에 맞춰 동류항끼리 묶고 다시 정리하는데(3번째 줄), 이때 올바르게 분류할 때의 리스크가 더 작은 것이 당연하므로 $$\lambda_{11}-\lambda_{21}<0$$이다. 따라서 부등호 방향이 바뀜에 유의하자(4번째 줄).

베이즈 정리를 활용하여 기존의 조건부 확률을 변환하고(5번째 줄), prior를 우측으로 곱하여 넘기면(6번째 줄) 좌변에 놓인 것은 베이지안 분류에서 봤던 likelihood ratio, 즉 상수값이 된다.

이제 양변에 로그를 취한 뒤 정리해주면 <a href="https://partial02.github.io/machinelearning/ML3/">이전 장</a>의 log likelihood ratio보다 $$log\frac{(\lambda_{22}-\lambda_{12})}{(\lambda_{11}-\lambda_{21})}:=\theta$$가 붙는데, 이를 theta로 정리해주자.

만약 $$\left| \lambda_{12} \right|>\left| \lambda_{21} \right|$$라 하면 진수의 분자 부분이 분모보다 커지므로 진수는 1보다 크고, theta는 양수가 된다. 따라서 우변이 더 커지므로 클래스를 2로 분류할 공산이 더 커진다.
이는 2를 1로 분류했을 때의 리스크 <strong class="r">$$\lambda_{12}$$가 더 크므로 2로 분류</strong>하는, 지극히 당연한 현상으로 볼 수 있다! 

### 베이즈 결정이론: 0/1 Loss의 경우

이번엔 다중 클래스 분류이지만 loss가 올바르게 분류하면 0, 아니라면 1로만 정해져있다고 하자. 쉽게 말하면

$$\lambda_{ik}=\begin{cases}0\quad\text{if }i=k\\1\quad \text{if }i\neq k\\ \end{cases}$$

이때 risk에 대한 식은 $$\lambda_{kk}=0$$이므로

R(\alpha_i|x)=\sum_k\lambda_{ik}{(c_k|x)}=\sum_{k\neq i}P(c_k|x)=1-P(c_i|x)

이고, risk가 최소가 되는 인덱스 i를 찾는 것은

$$arg\min_iR(\alpha_i|x)=arg\min_i[1-P(c_i|x)]=arg\max_iP(c_i|x)$$

다음과 같이 음의 부호를 없애는 과정에서 arg max로 바뀐다. 즉 risk가 최소가 되는 인덱스를 찾는 과정에서, 확률이 최대가 되는 인덱스를 찾는 Maximum posterior probability classification으로 바뀐 것 뿐이다. 이름만 거창하다.

결론을 말하자면, 0/1 Loss와 같이 <strong class="r">모든 에러의 경중이 같다면, minimum risk나 maximum posterior이나 같은 이야기</strong>가 된다는 것.

### 베이즈 결정이론: Reject action의 경우



<strong class="r"></strong>


