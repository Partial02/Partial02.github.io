---
layout: single
title: "[기계학습 3주차B] 나이브 베이즈 분류"
categories: MachineLearning
tag: [ML, COSE362]
toc: true # table of contents
author_profile: false # 각 콘텐츠에서 프로필 여부
sidebar: # 사이드바 설정
    nav: "docs"
search: true # 검색 여부 설정
---
<head>
    <!-- Latex -->
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<style>
    th, td {
        text-align: center;
    }
    .r {
        color: red;
    }
</style>

다음과 같이 남녀의 키에 대한 확률 표가 있다고 하고 베이지안 분류를 해보자.

![height]({{site.url}}/images/MacLea/mf_height.png)

이때 키가 170인 사람을 어느 성별로 분류해야할까? 조건부 확률이나 베이즈 정리 어느쪽으로 풀든 남자로 분류하는 것이 합당하다. 그런데 조건부 확률이 존재하지 않는 경우가 있을 수 있다. 

![height_weight]({{site.url}}/images/MacLea/mf_height_weight.png)

다음과 같이 남녀의 키와 몸무게에 대한 데이터셋에서 키가 170인데 몸무게가 50인 경우를 찾을 수 있는가? 없다. 이 경우는 학습 데이터에 해당 케이스가 없다. 그렇다면 이 문제는 ill-posed problem이기에 풀지 못하는 걸까?

우리는 2장에서 <a href="https://partial02.github.io/machinelearning/ML2/">ill-posed problem을 귀납적 편향(inductive bias)으로 풀 수 있다</a>고 배웠다! 이 귀납적 편향을 바탕으로 베이즈 정리를 통해 우리가 알지 못하는 미래의 데이터셋에 대한 확률 및 분류를 시행할 수 있다.

## 나이브 베이즈 분류(Naive Bayes' Classifiers; NBC)

귀납적 편향으로 각 입력값들이 키와 몸무게에 대한 클래스에 대하여 <strong class="r">조건부 독립(conditional independence)</strong>이라는 가정을 하자. 그렇다면 결합 확률(joint)을 주변 확률(marginal)의 곱으로 풀 수 있다. 이 방식을 통하여 우리가 알지 못하는 값을 예측하는 것이다.

이렇게 조건부 독립을 가정하여 베이즈 분류를 하는 것을 <strong class="r">나이브 베이즈 분류(Naive Bayes' Classifier)</strong>라 한다. 예를 들어, 키가 170이고 몸무게가 50일 때의 성별을 분류하는데 해당 케이스가 없다고 하자.

$$P(c_1|x_1=170,x_2=50) >?< P(c_2|x_1=170,x_2=50)$$

라는 문제 상황에서 베이즈 정리를 적용한 후 argmax를 취해주면

$$P(x_1=170,x_2=50|c_1)P(c_1) >?< P(x_1=170,x_2=50|c_2)P(c_2)$$

여기서 조건부 독립을 가정했으므로, 양변을 독립에 해당하는 곱셈의 꼴로 풀어주면

$$P(x_1=170|c_1)P(x_2=50|c_1)P(c_1)=0.025 < 0.05=P(x_1=170|c_2)P(x_2=50|c_2)P(c_2)$$

이므로 170에 50인 사람은 남자(c2 클래스)로 분류된다!

### NBC 예: 스팸 필터

bag of words로 처리된 d차원의 벡터가 있다고 하자. 이제 N개의 이메일에 대해서 N개의 binary 벡터에 각 이메일에 해당하는 단어가 있으면 1, 없으면 0으로 표기를 하고, 마지막에는 해당 이메일이 스팸인지 아닌지의 여부를 레이블로 표기하자.

이 경우 스팸 메일인 경우의 확률 P(c=1)과 그렇지 않은 경우의 확률 P(c=0)을 토대로 likelihood(우도)를 계산할 수 있다. 자세한 내용은 수식을 보면 쉽게 감이 올 것이다.

![spam]({{site.url}}/images/MacLea/NBC_spam.png)

### 나이브 베이즈 분류의 한계

나이브(naive)라는 명칭에서 눈치를 챘겠지만, 이 분류에는 한계가 세 가지가 있다.

<div class="notice--warning">
<h4>Limitation of Naive Bayes' Classifiers</h4>
<ol>
<li>곱해지는 확률 중 하나라도 0이면 싹 다 0이 되어버린다(Probability being zero)</li>
<li>연속 확률 분포를 처리할 수 없다(Discrete-valued input vectors)</li>
<li>독립을 가정할 수 없는 경우에도 독립을 가정해야 한다(Conditionally independent attributes)</li>
</ol></div>

다음 주(4주차)부터 이 문제를 해결한 분류기를 배울 것인데, 오늘은 먼저 1번을 해결할 수 있는 단순한 트릭까지만 소개할 것이다.

#### NBC 한계 1 해결: 로버스트 추정(Robust Estimation)

통계학에서 robust하다(=강직하다)라는 것은 <strong class="r">이상치에 영향을 적게 받는다</strong>라는 뜻이다. 여기서는 한계 1번인 probability being zero를 해결한다는 의미로 robust하다는 표현을 쓴 것이다.

조건부 확률에 대한 prior의 추정치 p와, 해당 prior에 대한 가중치 m을 적절히 활용하여 앞선 NBC의 수식을 다음과 같이 고친다.

$$P(x_i=v_j|c_k)=\frac{\sum_tr_t^k1(x_i^t=v_j)+m\cdot p}{N_k+m}$$

m과 p는 0이 아니므로 확률이 0이 될 가능성은 없앴다. 여기서 m과 p는 실제로 관측한 값은 아니지만, 표본이 커지면 이렇게 수렴할 것이라고 예측되는 값이다. 조금 억지처럼 보이지만 알고보면 근거가 있는 방법이라고.

이외의 해결책들은 다음 장에서 다룬다.


출처: 기계학습(COSE362) ㅇㄷㅅ 교수님
