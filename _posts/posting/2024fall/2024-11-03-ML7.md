---
layout: single
title: "[기계학습 9주차] 퍼셉트론"
categories: MachineLearning
tag: [ML, COSE362]
toc: true # table of contents
author_profile: false # 각 콘텐츠에서 프로필 여부
sidebar: # 사이드바 설정
    nav: "docs"
search: true # 검색 여부 설정
---
<head>
    <!-- Latex -->
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<style>
    th, td {
        text-align: center;
    }
    .r {
        color: red;
    }
</style>

4주차가 넘어갈 때부터 과제가 계속 나오더니, 시험공부도 하랴 과제도 하랴하다보니 정리를 멈추게 되었다. 나의 생존을 위한 어쩔 수 없는 선택.. 대신 시험은 다 잘 본 것 같다! 기계학습 80점 만점에 58점으로 그리 높은 점수는 아니지만, 그래도 80명 중에 9등! 만족스러운 등수다.

7, 8번 문제를 모두 0점 처리를 받았는데, 7번은 이해가 가지만 8번은 식까지 다 적었는데.. 적용하여 x2>1.5라는 것을 안 적었다는 이유로 0점을 받았다. 물론 난 그렇게 적용되는 지를 정확히 모르고 있었기에 억울하지는 않다! 그냥 내가 공부 안 한 거지 뭐~

끝까지 가서 이 힘겨운 산을 넘어가는 보자! 여기부턴 최대한 작성 시간을 줄이기 위해 왠만하면 그림 없이 텍스트로 요점만 적겠다.

## Oct 28 (Mon)

### 의사결정트리의 응용

먼저 의사결정트리는 중간 노드들의 해석과정을 정확히 알 수 없는 머신러닝이나 딥러닝(얘는 아예 불가)과는 달리, 노드의 if-then rule들을 통해 해석이 가능하다. 이때 해당 leaf node가 많은 원소를 갖고 있다는 것은, 해당 노드는 신뢰할 수 있다는 뜻이다. 이를 rule support(규칙 지지집합?)이라 한다.

또 인공지능에서 배웠듯 pruning이 가능하다. 이건 중간고사 범위였으니 일단 패스.

또한 먼저 적용된 rule일 수록, 즉 루트노드에 가까울 수록 중요한 특성일 것이다.(rule support가 클테니) 또한 사용하지 않거나 리프노드에 가깝게 사용되는 특성이 있다면 차원축소를 통해 해당 데이터(혹은 항)을 날려서 모델의 복잡도를 낮출 수도 있다. 이를 feature selection이라 한다.

또한 decision tree는 가까운 값들의 위치를 rule들을 통해 빠르게 추정할 수 있는데, 이를 fast localization이라 한다.

한편 각 rule을 하나의 단일변수가 아닌, 두 개 이상의 다변수로 설정할 수도 있다. 이를 다변량 트리(Multivariate Trees)라 한다. 여러 변수의 선형 결합을 기준으로 사용하는데, 우리가 이전에 가우시안 파트에서 공분산이 같을 때나 다를 때 사용했던 QDA가 이 다변량 트리와 유사한 예이다. QDA처럼 식을 복잡하게 만들 수도 있지만, decoding overhead가 크다고 한다.

다중 트리(Multiple Decision Trees)도 있는데, 이는 여러 의사결정트리를 합친 것으로 decision forest라고도 한다. 이중 매 데이터셋마다 <strong class="r">feature를 랜덤하게 선택하여</strong> 각각 decision tree를 만들고, 각 트리의 결과를 종합하여 판단하는 방법도 있는데, 이를 <strong class="r">Random Forest(랜덤 포레스트)</strong>라고 한다.

흔히들 sample을 random하게 뽑는 걸로 착각할 수 있는데, sample이 아니라 feature다. 각 특성을 랜덤하게 뽑음으로써 구성하는 것이 랜덤 포레스트 기법이다.

### Perceptron(퍼셉트론)

우리의 뇌를 구성하는 기본 단위인 뉴런(neuron)을 모방하여 1943년에 McCulloch와 Pitts가 고안한 방식이 <strong class="r">Perceptron(퍼셉트론)</strong>이다. 입력벡터 x의 각 input에 대해 가중치 w를 선형결합 방식으로 가중합을 시켜 출력값 y로 보내는 형식이다. 이진분류(LDA)의 경우 hard threshold라고 해서 양수면 1을, 음수면 0으로 통일하는 경우도 있다.

하지만 그것보다는 시그모이드 함수(Sigmoid function)가 많이 쓰이는데, 선형결합 -w^Tx가 양수면 0.5보다 큰 값을, 음수면 0.5보다 작은 값을 출력함으로써 positive/negative class에 들어갈 확률을 만드는 함수이다. 이때 기준이 되는 bias는 보통 0으로 잡는다.

한편 다중분류를 위해선 multiple perceptrons를 사용하는 경우도 있는데, 각 i번째 클래스에 들어갈 확률값 y_i를 합쳐서 출력벡터 y를 만드는 방식이다. 이때 입력은 여전히 x벡터이지만, 가중치는 각 퍼셉트론별로 쌓여서 가중치 행렬이 된다.

이후 출력벡터의 원소 y_k중 가장 큰 k번째 클래스로 입력값 x를 분류하는 것이 multiclass classification(다중 분류)가 된다.

한편 다중 분류에서도 확률을 만들어낼 수 있는데, 앞선 시그모이드 함수를 변형한 소프트맥스 함수(Softmax function)이다. k개의 클래스 중 i번째 클래스에 입력 x가 포함될 확률을 0과 1 사이에서 뱉는 함수로, 항상 소프트맥스 값들의 합은 1이 된다. 입력값으로는 선형결합 s_i=w_i^Tx를 받는다.

여기서 두 함수의 차이는 <strong class="r">시그모이드는 이진분류의 확률을, 소프트맥스는 다중분류의 확률을 출력</strong>한다는 것이다!

## Oct 30 (Wed)

### 퍼셉트론의 파라미터 학습

기본적으로 SGD(확률적 경사하강법) 과정을 전제로 한다. 즉 매번 랜덤한 표본의 경사를 활용하여 학습률만큼 곱하여 이동하는 방식이다. 그런데 왜 학습률을 곱하여서 최소점을 찾을까? 중간고사 범위의 회귀(regression)에서는 분명 그래디언트가 0이 되는 지점을 찾았다. 왜 SGD에선 그러지 않지?

이유는 크게 세 가지인데, 0이 되는 지점을 찾으려면 계산량이 너무 많아지기도 하고, step size만큼만 움직임으로써 local minimum이 아닌 global minimum을 찾으려 갈 수도 있으며, noise의 영향을 덜 받기도 하기 때문이다!

#### 로짓을 통한 단일 분류

먼저 단일 입력에 대한 에러 학습 과정을 보자. 손실 함수는 MSE(잔차제곱평균) 방식과 동일하게 하나의 정답 r과 하나의 출력 y간의 MSE로 계산된다. 이때 x벡터의 j번째 input에 대한 j번째 가중치 w_j로 에러함수를 편미분하면 기울기가 나온다.

우리는 기울기의 반대방향으로 이동하여야 국소 최소점으로 갈 수 있으므로, 마이너스 부호를 버리고 대신 학습률(learning rate; step size)을 곱하여 점진적으로 이동한다.

#### 로짓을 통한 다중 분류

이번엔 다중 입력의 경우를 보자. 다중 퍼셉트론에는 모두 동일한 벡터 x가 입력되지만, 각 퍼셉트론의 출력 y_i에는 가중치 w_ij가 관여한다는 것이 앞선 사례와의 차이이다.

손실함수는 모든 클래스에 대하여 k번째 클래스에서의 정답(0 혹은 1) r_k와 출력값 y_k의 차이를 MSE로 계산한 것이다. 이때 w_ij로 손실함수를 편미분하면 앞선 식과 비슷하지만 클래스 i가 추가된 식이 계산된다.

#### 시그모이드를 통한 단일 분류

이제부터는 활성화함수까지 거친 출력값을 토대로 update 식을 꾸려보겠다. 이때 총체적인 loss function은 14장의 교차 엔트로피(cross entropy)를 활용한다. 이게 가능한 이유는, 시그모이드/소프트맥스를 통과하면서 각 로짓들이 0과 1 사이의 확률값으로 단일화되기 때문이다! 확률분포가 나왔으니, 그 분포의 차이는 교차 엔트로피를 통해 계산할 수 있다!

이때 정답 분포 r과 출력 벡터 y의 차이가 클수록 교차 엔트로피 값은 커진다. 따라서 교차 엔트로피 값을 최소로 만드는 가중치의 값을 찾는 SGD의 과정과 논리가 같아진다. 본격적으로 계산 과정을 보자.

손실함수는 r과 y의 분포 차이(교차 엔트로피)로 정의되며, 이를 w_i로 편미분하면 신기하게도 '로짓을 통한 단일 분류'와 동일한 식이 도출된다. 그렇다면 이진 분류(단일 분류)를 진행할 때 굳이 활성화함수를 통과시킬 필요는 없을 것이다. 다중 분류에서도 이와 같은 결론이 나올까?

#### 소프트맥스를 통한 다중 분류

다중 퍼셉트론으로부터 계산된 로짓(logit)을 s_i, 이 로짓을 소프트맥스에 통과시킨 출력값을 y_i, 각 클래스에 대한 원-핫 벡터 값을 r_i라고 하면, 손실 함수(=cross entropy)는 다음과 같이 정의된다.

이때 w_ij(i번째 클래스의 퍼셉트론에 입력된 j번째 input에 해당하는 가중치)로 손실함수를 편미분하면 모든 계산과정을 거쳐 다중 분류 때의 update식과 동일한 식이 도출된다.

그렇다! update 식을 구할 땐 <strong class="r">활성화함수를 통과시켰든 아니든 똑같은 update 식</strong>이 도출된다! (Q: 내 추측임. 맞는지 확인할 것! ) 

### Multilayer Perceptron(다층 퍼셉트론; MLP)

단일 퍼셉트론으로는 불리안 AND, OR, NOT 등을 쉽게 표현할 수 있다. 그러나 1969년 Marvin Minsky는 기존 단일 퍼셉트론의 Hard Thresholding으로는 XOR를 구현할 수 없음을 역설하였다. 즉 linearly seperable(선형 구분 가능)하지 않다! 이것이 AI의 1차 겨울을 불러왔다.

이를 해결한 방법이 퍼셉트론의 출력값을 또 다른 퍼셉트론의 입력값으로 먹이는 것, 즉 <strong class="r">다층 퍼셉트론(MLP: Multilayer Perceptron)</strong>이다. XOR 문제는 이층 퍼셉트론을 통해 극복이 가능하다.

이때 중간 노드(하위 퍼셉트론의 출력값)를 은닉층(hidden layer)이라고 하며, 보통 비선형성을 부여하기 위해 시그모이드 함수를 활성화 함수로 채택한다. 가장 상위의 퍼셉트론의 출력 노드는 출력층(output layer)이라고 하는데 이때는 확률값을 리턴하기 위해 소프트맥스를 사용한다. 이때 회귀에서는 계산값인 로짓(logit) 자체를 출력으로 삼기도 한다.

다만 MLP에서 중요한 지점은, 은닉층에서 선형 함수를 사용할 경우 은닉층이 얼마나 깊어지든, 결국 하나의 층으로 축약이 된다는 것이다. 이는 linear combination을 특성으로 갖는 선형대수적인 차원의 문제인데, 두 행렬의 곱은 하나의 행렬로 표현이 가능하기에, 선형 결합이 가능한 함수로는 층의 깊이(depth)를 주기 어렵다. 따라서 <strong class="r">은닉층의 활성화함수는 반드시 비선형 함수</strong>여야 한다.

#### 보편 근사 정리(Universal Approximation Theorem)

MLP가 강력한 이유는 <strong class="r">MLP를 통해 어떠한 연속 함수도 표현 가능</strong>하다는 게 입증되었기 때문이다. 이를 보편 근사 정리라고 부른다. 정확히는 이층 퍼셉트론으로는 어떠한 연속함수도 표현이 가능하고, 삼층 퍼셉트론으로는 어떤 집합이든 모두 표현이 가능하다.

기존의 단일 퍼셉트론으로는 linear한 구별(half-plane)만이 가능했지만, two layers에서는 piece-wise하게, 즉 convex나 concave 중 하나를 표현이 가능해지고, three layers에서는 마지막에 or를 취함으로써 임의의 도형을 모두 합쳐서 표현할 수 있게 된다.

이는 푸리에 정리(모든 연속 함수는 sin과 cos의 선형결합으로 표현 가능)로 증명이 가능하며, 시벤코 정리(Cybenko's Theorem)로도 알려져 있다.


출처: 기계학습(COSE362) ㅇㄷㅅ 교수님
