---
layout: single
title: "[기계학습 9주차] 퍼셉트론"
categories: MachineLearning
tag: [ML, COSE362]
toc: true # table of contents
author_profile: false # 각 콘텐츠에서 프로필 여부
sidebar: # 사이드바 설정
    nav: "docs"
search: true # 검색 여부 설정
---
<head>
    <!-- Latex -->
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<style>
    th, td {
        text-align: center;
    }
    .r {
        color: red;
    }
</style>

4주차가 넘어갈 때부터 과제가 계속 나오더니, 시험공부도 하랴 과제도 하랴하다보니 정리를 멈추게 되었다. 나의 생존을 위한 어쩔 수 없는 선택.. 대신 시험은 다 잘 본 것 같다! 기계학습 80점 만점에 58점으로 그리 높은 점수는 아니지만, 그래도 80명 중에 9등! 만족스러운 등수다.

7, 8번 문제를 모두 0점 처리를 받았는데, 7번은 이해가 가지만 8번은 식까지 다 적었는데.. 적용하여 x2>1.5라는 것을 안 적었다는 이유로 0점을 받았다. 물론 난 그렇게 적용되는 지를 정확히 모르고 있었기에 억울하지는 않다! 그냥 내가 공부 안 한 거지 뭐~

끝까지 가서 이 힘겨운 산을 넘어가는 보자! 여기부턴 최대한 작성 시간을 줄이기 위해 왠만하면 그림 없이 텍스트로 요점만 적겠다.

## Oct 28 (Mon)

### 의사결정트리의 응용

먼저 의사결정트리는 중간 노드들의 해석과정을 정확히 알 수 없는 머신러닝이나 딥러닝(얘는 아예 불가)과는 달리, 노드의 if-then rule들을 통해 해석이 가능하다. 이때 해당 leaf node가 많은 원소를 갖고 있다는 것은, 해당 노드는 신뢰할 수 있다는 뜻이다. 이를 rule support(규칙 지지집합?)이라 한다.

또 인공지능에서 배웠듯 pruning이 가능하다. 이건 중간고사 범위였으니 일단 패스.

또한 먼저 적용된 rule일 수록, 즉 루트노드에 가까울 수록 중요한 특성일 것이다.(rule support가 클테니) 또한 사용하지 않거나 리프노드에 가깝게 사용되는 특성이 있다면 차원축소를 통해 해당 데이터(혹은 항)을 날려서 모델의 복잡도를 낮출 수도 있다. 이를 feature selection이라 한다.

또한 decision tree는 가까운 값들의 위치를 rule들을 통해 빠르게 추정할 수 있는데, 이를 fast localization이라 한다.

한편 각 rule을 하나의 단일변수가 아닌, 두 개 이상의 다변수로 설정할 수도 있다. 이를 다변량 트리(Multivariate Trees)라 한다. 여러 변수의 선형 결합을 기준으로 사용하는데, 우리가 이전에 가우시안 파트에서 공분산이 같을 때나 다를 때 사용했던 QDA가 이 다변량 트리와 유사한 예이다. QDA처럼 식을 복잡하게 만들 수도 있지만, decoding overhead가 크다고 한다.

다중 트리(Multiple Decision Trees)도 있는데, 이는 여러 의사결정트리를 합친 것으로 decision forest라고도 한다. 이중 매 데이터셋마다 <strong class="r">feature를 랜덤하게 선택하여</strong> 각각 decision tree를 만들고, 각 트리의 결과를 종합하여 판단하는 방법도 있는데, 이를 <strong class="r">Random Forest(랜덤 포레스트)</strong>라고 한다.

흔히들 sample을 random하게 뽑는 걸로 착각할 수 있는데, sample이 아니라 feature다. 각 특성을 랜덤하게 뽑음으로써 구성하는 것이 랜덤 포레스트 기법이다.

### Perceptron(퍼셉트론)

우리의 뇌를 구성하는 기본 단위인 뉴런(neuron)을 모방하여 1943년에 McCulloch와 Pitts가 고안한 방식이 <strong class="r">Perceptron(퍼셉트론)</strong>이다. 입력벡터 x의 각 input에 대해 가중치 w를 선형결합 방식으로 가중합을 시켜 출력값 y로 보내는 형식이다. 이진분류(LDA)의 경우 hard threshold라고 해서 양수면 1을, 음수면 0으로 통일하는 경우도 있다.

하지만 그것보다는 시그모이드 함수(Sigmoid function)가 많이 쓰이는데, 선형결합 -w^Tx가 양수면 0.5보다 큰 값을, 음수면 0.5보다 작은 값을 출력함으로써 positive/negative class에 들어갈 확률을 만드는 함수이다. 이때 기준이 되는 bias는 보통 0으로 잡는다.

한편 다중분류를 위해선 multiple perceptrons를 사용하는 경우도 있는데, 각 i번째 클래스에 들어갈 확률값 y_i를 합쳐서 출력벡터 y를 만드는 방식이다. 이때 입력은 여전히 x벡터이지만, 가중치는 각 퍼셉트론별로 쌓여서 가중치 행렬이 된다.

이후 출력벡터의 원소 y_k중 가장 큰 k번째 클래스로 입력값 x를 분류하는 것이 multiclass classification(다중 분류)가 된다.

한편 다중 분류에서도 확률을 만들어낼 수 있는데, 앞선 시그모이드 함수를 변형한 소프트맥스 함수(Softmax function)이다. k개의 클래스 중 i번째 클래스에 입력 x가 포함될 확률을 0과 1 사이에서 뱉는 함수로, 항상 소프트맥스 값들의 합은 1이 된다. 입력값으로는 선형결합 s_i=w_i^Tx를 받는다.

여기서 두 함수의 차이는 <strong class="r">시그모이드는 이진분류의 확률을, 소프트맥스는 다중분류의 확률을 출력</strong>한다는 것이다!

## Oct 30 (Wed)

