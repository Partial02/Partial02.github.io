---
layout: single
title: "[회귀분석 1주차] 어쩌다보니 한 주간 수업만 3번"
categories: Statistics
tag: [STAT, STAT342]
toc: true # table of contents
author_profile: false # 각 콘텐츠에서 프로필 여부
sidebar: # 사이드바 설정
    nav: "docs"
search: true # 검색 여부 설정
---
<head>
    <!-- Latex -->
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<style>
    th, td {
        text-align: center;
    }
    .r {
        color: red;
    }
</style>

수강신청 때 ㅈㅇㅅ 교수님 수업을 잡아서 열심히 들으려고 했고, 실제로 월/수 수업을 들으면서 되게 좋으신 분이라는 생각을 많이 했다. 이분 수업을 끝까지 듣고 싶었으나..

정정 과정에서 월/수 1교시인 기계학습을 잡기 위해 ㅊㅅㅂ 교수님의 금요일 3시간 연강 수업으로 수강을 옮기게 되었다. 다행인 건? ㅊㅅㅂ 교수님 수업도 정말 좋다! 잘 가르치시고 자신이 가르치는 과목에 자신이 있으신 게 보인다!

어떻게 한 과목을 가르치는 두 교수님이 모두 좋을 수 있지? 쉽지 않은 일이다. 대학에 진학하게 된 목적 중 하나인 회귀분석, 드디어 내가 간다!

## 회귀분석이 뭔진 알고 있지만

사실 첫 수업은 1시간만에 끝내주셔서 크게 특기할 만한 내용도, 모르는 내용도 없었다. 2주차 수업은 상당히 내용도 많고 신기한 수식도 많았는데, 1주차야 뭐 첫 수업이라 평이하게 진행하신 듯

회귀 분석(Regression Analysis)은 <strong class="r">변수 간의 함수적 관계</strong>를 조사(investigating functional relationships)하는 방법이다. 회귀분석의 기본 포인트는 '어떻게 회귀선을 찾을지'와 '그 회귀가 얼마나 신뢰할만 한지' 등이다.

![linear]({{site.url}}/images/RegAna/linear.png)

독립 변수 X(Independent variable; feature)에 대한 종속 변수 Y(Dependent variable; target/response variable)에 대하여 함수적 관계인 회귀선(regression line; line of best fit: 적합선)을 찾는 것이 목적이다.

이때 산점도 상의 데이터는 회귀선을 기준으로 신뢰 구간 안의 upper bound와 lower bound 사이에 있으며, 이는 R^2이라는 기준으로 판단할 수 있다. 회귀선에서 지나치게 멀리 떨어진 데이터는 이상치(outlier)라 한다.

여기서 X는 예측 변수(predictor variable)이라고도 불리며, 조금 어렵게는 공변량(covariate), 회귀변수(regressors), 혹은 컴퓨터 분야에서 피처(features)라고 불린다.

Y는 응답 변수(response variable)이 정식 명칭이며, 이외에도 결괏값(output)이나 타겟(target)이라고 불린다. 말 그대로 <strong class="r">Y <- X</strong>의 함수적 관계

## 선형 회귀(Linear Regression)

회귀는 다시 선형 회귀와 비선형 회귀로 나뉜다. 여기서 선형이라 함은 additive, 즉 합으로 연결된 식인지를 보는 것이다(아주 간략하게 말하자면 그렇다).

$$Y=\beta_0+\beta_1X_1+\beta_2X_2+\cdots \beta_pX_p+\epsilon$$

X라는 data가 들어와서 함수적 관계를 갖춘 Y로 변환되는 과정인데, 이 data는 우리가 눈여겨볼 만한 signal과 그렇지 않은 noise가 있다. 우리는 이 signal을 X라는 예측 변수에 보내고 나머지 noise를 error(epsilon)로 보낼 것이다.

여기서 베타로 표현된 것은 <strong class="r">회귀 계수</strong>(regression coefficients)라고 부르는데 이것은 하나의 모수(parameter)로 우리가 알지 못하는 값이다. 후에 이 모수 대신 추정량을 계산하여 사용할 것이다.

선형 회귀의 장점은 해석이 용이하고, 선형적이지 않은 모델을 선형적으로 끌어내려서 해석이 가능하다는 것이다(아마 다음 포스팅에 이 내용이 있을 것이다)

선형 회귀는 다시 단순 선형 회귀(SLR: Simple Linear Regression)와 다중 선형 회귀(MLR: Multiple Linear Regression)로 나뉘는데, 이는 공변량 X의 개수에 따라 달라진다.

X가 하나면 단일 예측 인자이므로 단순 선형 회귀, X가 두 개 이상이면 다중 선형 회귀가 된다.

### 근데 이름이 왜 회귀야? 무슨 연어야?

'Regression'이라는 용어는 원래 '퇴보'라는 뜻이다. 뭘로 퇴보했다는 걸까? 이걸 찾기 위해선 프란시스 갈턴(Francis Galton)의 '부자간 키 데이터'를 봐야한다.

![galton]({{site.url}}/images/RegAna/galtonData.png)

이 산점도의 x축과 y축의 60과 75지점에 총 4개의 점선을 그려보자. 아빠의 키가 60인치보다 작을 경우 그 아들의 키는 모두 60인치를 넘었다. 반대로 아빠의 키가 75인치보다 큰 경우, 아들의 키는 모두 75인치보다는 작아졌다. 이게 무슨 뜻이냐.

모든 <strong class="r">유전형질은 무한히 커지지 않고 평균으로 퇴보</strong>한다는 것이다. 키 큰 유전자가 아무리 대물림되더라도 수십 대가 지나면 결국 평균으로 가있다는 것이다. 이래서 Regression이라는 이름이 붙여졌다. 평균으로 퇴보해서.

## 그래프 등 기타 내용

먼저 산점도가 나오는데 요즘은 중3때 배우는 내용이라 굳이 설명할 필요가 없다. 양의 상관, 음의 상관 등등.

하나 볼만한 것은 범주형 데이터의 표현인데, 수치형 데이터는 보통 연속적(continous)한 반면, 범주형(categorical) 데이터는 하나 이상의 축이 양자화(급간이 정확히 띄어져있음)돼있기 때문에 산점도보다는 상자그림이 더 좋다는 결론

별개로 비선형 회귀가 더 좋을 때도 있다. 선형 회귀로 표현하면 안되는 데이터도 존재하기 마련! 또한 시계열 데이터도 회귀로 다룰 수 있다는 내용이 있었다.


출처: 회귀분석(STAT342) ㅊㅅㅂ 교수님
