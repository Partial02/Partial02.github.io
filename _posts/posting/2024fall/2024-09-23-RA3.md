---
layout: single
title: "[회귀분석 3주차] 교란변수 및 t검정"
categories: Statistics
tag: [STAT, STAT342]
toc: true # table of contents
author_profile: false # 각 콘텐츠에서 프로필 여부
sidebar: # 사이드바 설정
    nav: "docs"
search: true # 검색 여부 설정
---
<head>
    <!-- Latex -->
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<style>
    th, td {
        text-align: center;
    }
    .r {
        color: red;
    }
    .b {
        color: blue;
    }
</style>

오늘도 아니나 다를까 매우 내용이 많고 복잡하다. 수리통계학 내용도 다수 나온다. 이제는 몇 시간을 투자하여 깃헙을 정리하지는 않기로 했으므로, 나중에 이해를 도울 수 있도록 흐름과 맥락 위주로 서술한다.

## 교란변수(Confounding Variables)

MLR에서 각각의 회귀계수에 대한 영향은, 나머지 X값을 바꾸지 않고 고정한 채, 해당 회귀계수에 곱해지는 X값만 바꿔야 정확히 확인할 수 있다. 이처럼 나머지 X값을 <strong class="r">고정(adjusted)</strong>해야하는 회귀계수를 <strong class="r">partial regression coefficients</strong>라 한다.

그 예제로 차량의 무게와 마력에 따른 제로백 시간에 대한 SLR, MLR이 주어져있다. 차량이 무거우면 제로백이 오래 걸리는 것이 당연하니 positive한 적합선이 나와야할 것 같지만, 실제 둘 간의 관계는 negative였다. 그런데 마력까지 고려하여 다중 회귀를 하면 다시 positive가 나온다. 무엇이 문제일까?

앞선 SLR은 accelation과 weight에 모두 영향을 주는 마력(horsepower)이라는 변수를 모델에 고려하지 않았기 때문에 발생한 문제이다(마치 no-intercept 모델을 남용하면 오류가 발생하듯이). 이렇듯 계산에 고려되지 않았지만, 나머지 두 변수에 모두 영향을 주는 변수를 <strong class="r">교란 변수</strong>(Confounding Variable)라 한다.

39페이지의 내용을 보면 알 수 있듯이, 제로백과 무게에 대한 전체적인 관계는 negative이지만, 마력에 따라 labeling(군집을 나눔)을 해보니 각각의 군집 내에서는 positive한 관계를 갖음을 볼 수 있다. 이게 옳은 해석이다. 이 교란변수의 영향을 무시하고 SLR을 하니 오류가 발생한 것이다.

이렇듯 세부적으로 보면 변수 간의 경향을 정확히 찾을 수 있으나, 전체 데이터를 나열함으로써 경향이 오히려 사라져버리는 현상을 <strong class="r">심슨의 역설</strong>(Simpson's Paradox)이라 한다.

## R의 tree 데이터 모델링

```R
data("trees")
```

라는 코드를 통해 R에 빌트인된 trees 데이터셋을 불러오자. 이 데이터셋은 각 나무들의 직경(Girth)과 높이(Height) 및 부피(Volume)에 대한 데이터를 저장하고 있다. 여기서 직경에 대한 부피의 산점도에 회귀선을 그어보자.

높이와 부피에 대한 산점도에서는 비교적 선형회귀가 잘 먹혔다. 물론 높이가 증가할수록 부피에 대한 변동(variability, 즉 퍼져있는 분산의 정도)이 증가하고 있긴 하다.

그런데 직경과 부피에 대한 산점도에서는 SLR의 선형 회귀보다, 이차곡선이 더 경향을 잘 설명하는 것을 볼 수 있다. 당연하다. 부피는 직경의 제곱에 비례할 테니까. 때로는 이러한 non-linearity 모델이 더 경향을 잘 반영할 수 있는 것이다.

두 경우 (즉, 선형회귀에서 점점 변동이 커지거나, 이차곡선이 경향을 더 잘 표현하는 경우)에는 x와 y에 로그를 취해, 더 나은 선형 회귀로 변환할 수 있다. 단 각 변수가 양수(positive)여야 로그를 취할 수 있을 것이다.

이 데이터셋에서도 교란변수 비슷한 상황을 볼 수 있는데, 다행히 심슨의 역설 상황까지는 아니다. 높이에 대한 부피의 산점도는 positive인데, R의 cut 함수를 통해 diameter를 5개의 구간으로 나누어 adjust 하더라도, 역시 각 직경의 구간 내에서 높이~부피가 positive함을 확인할 수 있다.

자 그렇다면

$$\text{Timber Volume}\approx \text{(constant)(Diameter)^2(Height)}$$

라는 사실을 알기에, log transformation을 해주면

$$\log{\text{(Volume)}}=\beta_0+\beta_1\log\text{(Diameter)}+\beta_2\log\text{(Height)}+\epsilon$$

라는 MLR 모델을 세울 수 있다. 이때 부피는 직경의 제곱에, 높이에 정비례하였으므로, <strong class="r">$$\beta_1=2,\beta_2=1$$</strong>라는 것을 쉽게 <strong class="r">추론</strong>할 수 있다. 그런데 이게 진짜 합당한 추리일까? 주어진 trees 데이터셋에서 이 값이 정말 합당한 값인지 구해보자. 먼저 코드로 두 intercept를 계산하면 다음과 같다.

```R
data("trees")
trees$Diameter = trees$Girth
# 변수 Girth를 Diameter라고 rename하였다

lmtrees = lm(log(Volume) ~ log(Diameter) + log(Height), data=trees)
lmtrees$coef
##  (Intercept) log(Diameter)   log(Height) 
##    -6.631617      1.982650      1.117123 
```

이 수치가 과연 합당한 값인지, LSE의 분산을 구함으로써 살펴보겠다.

## 표준 오차(Standard Errors)

먼저 LSE인 $$\hat\beta$$가 불편추정량(unbiased estimator)임을 보이자. r.v인 Y와 epsilon에 비해 X는 값이 관측된 fixed numbers(쉽게 말해, 상수 취급)이고 beta는 파라미터(모수이므로 상수)라는 것으로 보일 수 있다. 계산을 다 해보면

$$E[\hat\beta]=\beta$$

라는 것을 쉽게 입증할 수 있다. 즉 LSE인 <strong class="r">beta hat은</strong> 불편추정량, 다시 말해 <strong class="r">consistent estimator</strong>임을 알 수 있다. consistent하다는 것은 확률적으로 특정 값에 수렴한다는 뜻이다(수리통계학 내용).


<strong class="r"></strong>












