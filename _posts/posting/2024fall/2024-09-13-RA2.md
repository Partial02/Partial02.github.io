---
layout: single
title: "[회귀분석 2주차] 선형회귀와 최소자승법"
categories: Statistics
tag: [STAT, STAT342]
toc: true # table of contents
author_profile: false # 각 콘텐츠에서 프로필 여부
sidebar: # 사이드바 설정
    nav: "docs"
search: true # 검색 여부 설정
---
<head>
    <!-- Latex -->
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<style>
    th, td {
        text-align: center;
    }
    .r {
        color: red;
    }
    .b {
        color: blue;
    }
</style>

## SLR(단순선형회귀)와 MLR(다중선형회귀)

### 뭐가 선형 회귀일까?

지난 장에서 기본적으로 회귀는 예측 변수 X에서 응답 변수 Y에 대한 함수적 관계 f를 찾는 것이라고 했다. 즉

$$Y=f(X_1,X_2,\cdots,X_p)$$

이고 함수 f는 흔히, 알지 못하는 상태로 주어진다. 이제 함수적 관계 f를 찾아가는 과정을 배우겠다.

여기서 X1부터 Xp까지의 data는 deterministic한 함수 f의 신호(signal)로 입력된다. 그리고 나머지 불확실성은 noise인 error(epsilon)이 먹게 된다. 따라서 이 신호를 얼마나 잘 살리느냐가 핵심인데, 이 신호를 살리기 가장 간단한 형태가 <strong class="r">선형 회귀</strong>이다.

그런데 넓은 의미에서 '선형성'이라는 것은 변수 X가 아닌 <strong class="r">회귀 계수와 종속 변수간의 선형성</strong>을 의미한다. 이게 무슨 의미냐고? 아래의 수식들이 사실 linear regression 모델이라는 뜻이다.

$$Y=\beta_0+\beta_1X+\beta_2$$<strong class="b">$$X^2$$</strong>$$+\epsilon$$

$$Y=\beta_0+\beta_1$$<strong class="b">$$log(X)$$</strong>$$+\epsilon$$

누가 봐도 비선형 모델 아닌가? 라고 생각했었다. 아니라 한다. 왜냐? 첫번째 식의 경우 변수 X^2을 그냥 X2라는 새로운 변수로 생각하면, 이 식은 다중 선형 회귀가 된다. 로그 X도 새로운 변수 X3로 치환해버리면 이 역시 선형성을 띠게 된다. 하나 못해 아래 식을 보라

### 비선형 모델을 선형 모델로 바꾸기

$$y=X^\beta\epsilon\text{ 에서 양변에 로그를 취하면}$$

$$log\;y=\beta log\;X+log\;\epsilon\text{ 이므로 }y'=\beta X'+\epsilon'$$

어라? 비선형 모델이 선형 모델로 뚝딱 바뀌었다.

$$Y=\frac{X}{\alpha X+\beta}$$

얘도 가능할까? 역수를 취한뒤 치환을 하면

$$\frac{1}{Y}=\alpha+\beta(\frac{1}{X})\text{ 에서 }Y'=\alpha+\beta X'$$

감쪽같다. 이번엔 경제학 분야에서 쓰이는 Cobb-Douglas production function을 살펴보자. 노동(L)과 자본(K)이 생산량(Y)을 결정한다는 의미의 함수이다.

$$V=\alpha K^{\beta_1}L^{\beta_2}\text{ 에서 양변에 로그를 취하면}$$

$$log(V)=log(\alpha)+\beta_1log(K)+\beta_2 log(L)$$

익히 봐서 알겠지만 로그꼴의 변수들을 새로운 변수로 정의하면 선형이 되어버린다. 자 이제 시사하는 바는 <strong class="r">선형 회귀는 강력하다</strong>는 것이다. 비선형 모델을 선형적으로 해석이 가능하고, 선형 회귀는 다루기 비교적 쉬우니, 우리는 선형 회귀부터 열심히 공부하면 된다!

다만 $$Y=\beta_0\beta_1^X+\epsilon$$처럼 회귀계수끼리의 곱은 선형 모델로 만들 수 없다. 로그를 취해봐도 안 됨을 알 수 있다. 아마 선형성이 '회귀계수와 종속변수'간의 것이라, 회귀계수끼리 곱해진 건 안되는 듯

### 다중 선형 회귀 모델(MLRM)로의 확장

![SMLR]({{site.url}}/images/RegAna/SLR_MLR.png)

회귀에서 각 row(혹은 pairs)는 case이다.(subject(피험체)이거나 individual(개개인)) record나 data point라고도 부른다. SLR(단순 선형 회귀)는 single input에 single response인 반면, MLR(다중 선형 회귀)는 multiple inputs이라는 차이가 있다.

$$\textbf{MLRM: }\;y_i=\beta_0+\beta_1x_{i1}+\cdots+\beta_px_{ip}+\epsilon_i\gets \text{SLRM if p=1}$$

MLRM에서 p=1이면 SLRM인 것처럼, MLRM은 SLRM의 확장이다. 엡실론이 정규분포 N(0,sigma^2)에서 iid하게 뽑혔다고 가정해보자. 즉 error의 분산은 시그마 제곱인 것이다.

여기서 절편(intercept) beta0이나 회귀계수(기울기) beta_k, 에러의 분산 sigma^2 등은 모수(parameters)이다. 원래부터 정해져있는, 랜덤 값이 아닌 상수라는 것. 신기한 점은 관측값 X도 모수라는 것이다.

그러나 에러 epsilon이나 출력 Y는 확률 변수(random variable, a.k.a 'r.v.')라는 점에 유의하자. 우리는 관측된 입력 변수 X, 출력 변수 Y를 통하여서, 알려지지 않은 회귀 계수들과 에러 및 에러의 분산 등을 찾아내야한다.

![MLRM]({{site.url}}/images/RegAna/MLRM_matrix.png)

벡터 X, Y, epsilon과 행렬 X로 MLRM(다중 선형 회귀 모델)을 표현하면 다음과 같다. n은 관측 값의 개수, p는 입력 하나 당의 특성치(features)이다.

중간의 n*(p+1)짜리 행렬 X를 <strong class="r">design matrix</strong>라 하는데, 여기서 1열의 1은 y절편과 곱해지는 곱셈의 항등원이고, 그 오른쪽은 변수 X에 대한 값들이다. 이 design matrix는 바로 옆의 벡터 beta와 곱해지는데, beta0은 절편을 결정하고, 나머지는 회귀계수들이 들어간다.

## 최소자승추정법(Least Squared Estimation)

이제 베타(회귀계수)값들을 찾아보자. 이제부터 사용할 방법은 LS라고 불리는 <strong class="r">Least Squares Method(최소자승법)</strong>이다. 뭐 철자가 조금씩 바뀌긴 하지만 통칭 LS라고 불린다.

![line]({{site.url}}/images/RegAna/regression_line.png)

여기서 빨간 회귀선은 $$y=\beta_0+\beta_1x+\epsilon$$의 식을 따른다. 이때 각 점들에 해당하는 y값들이 회귀선 위에 있다면 에러(error)가 양수, 아래에 있다면 에러가 음수이다. 여기서 핵심은, 각 점들로부터의 에러가 최소가 되는 회귀선을 찾는 것이다. 그렇다면 모든 에러의 합이 최소가 되도록 하는 회귀선을 찾으면 될까?

그렇지 않다. 적절한 회귀선을 찾았다면 <strong class="r">에러의 단순합은 반드시 0</strong>이 된다. 즉 $$\sum_i\epsilon_i=0$$인 것으로, 단순합으로는 아무런 정보를 얻을 수가 없다는 사실을 알 수 있다. 따라서 우리는 제곱합을 사용하는데, 
이를<strong class="r">SSE</strong>(Sum of Squared Error)라고 부른다. 따라서 SSE는

$$SSE=\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)^2=\sum_{i=1}\epsilon_i^2$$

다음과 같이 각 포인트 y에서 회귀선의 fitted value인 y hat를 뺀 값들의 제곱합임을 알 수 있다. 그렇다면 SSE가 최소가 되게 하는 회귀계수를 찾아야하는데, 제곱의 최소를 찾는다? 답이 보이는가? 각 회귀계수에 대하여 SSE를 <strong class="r">편미분</strong>하면 된다! 이렇게 편미분을 통해 나온 회귀계수 <strong class="r">$$\hat\beta$$가 LSE</strong>(Least Squared Estimator)다. 추정량이기에 hat 기호로 표기함을 확인하자.

### SLR에서의 LSE 찾기

이제 LSE가 어떤 값이 나와서 회귀선이 어떻게 그려지는 지를 알아볼 것이다. 앞으로는 (1) 정규방정식을 통한 풀이 / (2) 선형대수학을 이용한 풀이 / (3) R 프로그래밍을 통한 풀이, 이렇게 3가지로 나눠서 각기 설명하겠다. (1)번과 (2)번은 서로 연관돼있어서 경계를 넘나들므로 두 풀이의 관계를 잘 살피며 이해해보자.

#### SLR에서의 LSE: 정규방정식을 통한 풀이

앞서 LSE를 다음과 같이 표기한다고 다뤘다.

$$L(\hat\beta_0,\hat\beta_1)=\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)^2$$

따라서 LSE에 대한 함수 L을 beta0과 beta1로 각각 편미분한 편미분방정식은

$$\frac{\partial L}{\partial \hat\beta_0}=-2\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)=0$$

$$\frac{\partial L}{\partial \hat\beta_1}=-2\sum_{i=1}^nx_i(y_i-\hat\beta_0-\hat\beta_1x_i)=0$$

여기서 알 수 있는 점은 (1) <strong class="r">잔차합(에러의 단순합)은 항상 0</strong>이라는 것과, <strong class="r">(x_i * 잔차)의 합도 항상 0</strong>이라는 것이다. 이처럼 편미분 방정식이 이변수이고, 식도 2개가 나와서 답이 나오는(solvable) 방정식을 <strong class="r">정규 방정식(Normal Equation)</strong>이라고 한다. 잘 기억해두자.

이 정규 방정식을 의미론적으로 해석하면

$$\sum e_i=\sum x_ie_i=0\Rightarrow$$ <strong class="r">$$x\perp\!\!\!\!\perp e$$</strong>

즉, signal X와 에러 e는 통계적으로 독립이라는 뜻이다. signal인 X에 의해 회귀계수인 beta들이 결정되고, 나머지 필요없는 noise들이 error에 들어간다는 종전의 이론에 정확히 맞아 떨어진다!!

이야기가 딴 데로 새버렸는데, 아무튼 이 두 식을 정리하면

$$n\hat\beta_0+\hat\beta_1\sum_{i=1}^nx_i=\sum_{i=1}^ny_i$$

$$\hat\beta_0\sum_{i=1}^nx_i+\hat\beta_1\sum_{i=1}^nx_i^2=\sum_{i=1}^nx_iy_i$$

이 나온다. 그런데 $$\sum_{i=1}^nx_i=n\bar{x},\sum_{i=1}^ny_i=n\bar{y}$$이므로 대입하여 식을 변환시키면

<strong class="r">$$\text{(1) }\hat\beta_0=\bar{y}-\hat\beta_1\bar{x}$$</strong>

$$\hat\beta_0n\bar{x}+\hat\beta_1\sum_{i=1}^nx_i^2=\sum_{i=1}^nx_iy_i$$

이다. (1)번식에서 beta 1 hat을 구하면 beta 0 hat이 구해짐을 알 수 있다. 이제 (1)번식을 아래 식에 대입하면

$$(\bar{y}-\hat\beta_1\bar{x})n\bar{x}+\hat\beta_1\sum_{i=1}^nx_i^2=\sum_{i=1}^nx_iy_i$$

$$\hat\beta_1(\sum_{i=1}^nx_i^2-n\bar{x}^2)=\sum_{i=1}^nx_iy_i-n\bar{x}\bar{y}$$

<strong class="r">$$\text{(2) }\hat\beta_1=\frac{\sum_{i=1}^nx_iy_i-n\bar{x}\bar{y}}{\sum_{i=1}^nx_i^2-n\bar{x}^2}:=\frac{A}{B}$$</strong>

이 도출된다. (2)번식을 잘 기억하자. 이제 2번식의 분자를 A, 분모를 B라 두고 계속 변환해 보겠다.

$$\sum(x_i-\bar{x})=0\longrightarrow \sum(x_i-\bar{x})\bar{y}=0$$

그 전에 다음 식을 하나 보자. i에 대해 이미 합이 0인 식이기에, 상수 y bar를 곱해도 똑같이 0이라는 사실에 주목하자. centered된 x와 y에 대하여 위 식을 사용하면 분자의 식 A는

$$\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})=\sum_{i=1}^n(x_i-\bar{x})y_i=\sum_{i=1}^nx_iy_i-n\bar{x}\bar{y}:=A$$

또한 아래 분모의 식 B는

$$\sum_{i=1}^n(x_i-\bar{x})^2=\sum_{i=1}^nx_i^2-n\bar{x}^2:=B$$

이므로 두 식 A와 B를 (2)번식에 대입하면

$$\hat\beta_1=\frac{\sum_{i=1}^nx_iy_i-n\bar{x}\bar{y}}{\sum_{i=1}^nx_i^2-n\bar{x}^2}=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}=\frac{\sum_{i=1}^n(x_i-\bar{x})y_i}{\sum_{i=1}^n(x_i-\bar{x})^2}$$

이고 특히 가운데 식에 주목하면

<strong class="r">$$\text{(3) }\hat\beta_1=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}=\frac{Cov(X,Y)}{Var(X)}$$</strong>

3번 식이 나온다! 이를 통해 slope(기울기)인 <strong class="r">beta1은 X의 분산에 대한 공분산 X, Y의 비율</strong>임을 알 수 있다!

<div class="notice--warning">
<h4>정리 1: 정규방정식을 통한 SLR에서의 LSE는 다음과 같이 구해진다</h4>
<ul>
<li>$$\hat\beta_0=\bar{y}-\hat\beta_1\bar{x}$$</li>
<li>$$\hat\beta_1=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}=\frac{Cov(X,Y)}{Var(X)}$$</li>
</ul></div>

#### SLR에서의 LSE: 선형대수학을 이용한 풀이

이 풀이는 후술할 'MLR에서의 LSE: 선형대수학을 이용한 풀이'에서 이어지는 내용이다. 아직 MLR 파트를 읽지 않았다면 아래로 내려가서 그 파트를 먼저 읽고, 다시 돌아오자.

MLR에서의 LSE인 beta hat은 다음과 같이 구할 수 있음을 보였다.

$$\hat\beta=(X^TX)^{-1}(X^Ty)$$

단순 선형 회귀(SLR)이므로 y와 X를 다음과 같이 정의해보자.

$$\begin{align}
y:=\begin{bmatrix} 
   y_1  \\
   y_2  \\
   \vdots \\
   y_n  \\
   \end{bmatrix},
X := \begin{bmatrix} 
   1 & x_1  \\
   1 & x_2  \\
   \vdots & \vdots \\
   1 & x_n  \\
   \end{bmatrix}
= \begin{bmatrix} 
   1_n ,& x  \\
   \end{bmatrix}
\end{align}$$

1_n과 X는 각각의 열벡터를 축약한 notation이다. 이때 X^T와 X의 행렬곱은

$$\begin{align}
X^TX = \begin{bmatrix} 
   1_n \\
   x \\
   \end{bmatrix}
\begin{bmatrix} 
   1_n & x  \\
   \end{bmatrix}
=\begin{bmatrix} 
   1_n^T1_n & 1_n^Tx  \\
   x^T1_n & x^Tx \\
   \end{bmatrix}
=\begin{bmatrix} 
   n & \sum x_i  \\
   \sum x_i & \sum x_i^2 \\
   \end{bmatrix}
\end{align}$$

여기서 역행렬 공식을 떠올려야 하는데, 2 by 2 행렬의 역행렬 공식은

$$\begin{align}
A = \begin{pmatrix} 
   a & b \\
   c & d \\
   \end{pmatrix} \to 
A^{-1}=\frac{1}{ad-bc}\begin{pmatrix} 
   d & -b  \\
   -c & a \\
   \end{pmatrix}
\end{align}$$

이므로 이를 적용하면

$$\text{(1) }\begin{align}
(X^TX)^{-1} = \frac{1}{n\sum x_i^2-(\sum x_i)^2}
\begin{bmatrix} 
   \sum x_i^2 & -\sum x_i  \\
   -\sum x_i & n \\
   \end{bmatrix}
\end{align}$$

한편 X^T와 y의 행렬곱은

$$\text{(2) }\begin{align}
X^Ty=\begin{bmatrix} 
   1_n^T  \\
   x^T  \\
   \end{bmatrix}y
=\begin{bmatrix} 
   1_n^Ty  \\
   x^Ty  \\
   \end{bmatrix}
=\begin{bmatrix} 
   \sum y_i  \\
   \sum x_iy_i  \\
   \end{bmatrix}
\end{align}$$

따라서 MLR에서의 $$\hat\beta=(X^TX)^{-1}(X^Ty)$$에 위 두 소결론 (1)번식과 (2)번식을 대입하면

$$\begin{align}
\hat\beta &= (X^TX)^{-1}(X^Ty)
\\&= \frac{1}{n\sum x_i^2-(\sum x_i)^2}
\begin{bmatrix} 
   \sum x_i^2 & -\sum x_i  \\
   -\sum x_i & n \\
   \end{bmatrix}
\begin{bmatrix} 
   \sum y_i  \\
   \sum x_iy_i  \\
   \end{bmatrix}
\\&=\frac{1}{n\sum x_i^2-(\sum x_i)^2}
\begin{bmatrix} 
   \sum x_i^2\sum y_i - \sum x_i\sum x_iy_i  \\
   -\sum x_i\sum y_i + n\sum x_iy_i \\
   \end{bmatrix}
\\&=\frac{1}{n(\sum x_i^2-n\bar{x}^2)}
\begin{bmatrix} 
   n(\sum x_i^2\bar{y} - \bar{x}\sum x_iy_i)  \\
   n(\sum x_iy_i - n\bar{x}\bar{y}) \\
   \end{bmatrix}
\end{align}$$

이므로 이를 정리하면

<strong class="r">$$\begin{align}
\hat\beta=
\begin{bmatrix} 
   \frac{\sum x_i^2\bar{y} - \bar{x}\sum x_iy_i}{\sum x_i^2-n\bar{x}^2} \\
   \frac{\sum x_iy_i - n\bar{x}\bar{y}}{\sum x_i^2-n\bar{x}^2} \\
   \end{bmatrix}
=\begin{bmatrix} 
   \hat\beta_0 \\
   \hat\beta_1 \\
   \end{bmatrix}
\end{align}$$</strong>

이다. 와! 진짜 오래 걸렸다. (증명보다 LaTex 작성이 더 오래 걸리는 현실..) 핵심은 벡터 beta의 1번째 원소에서 beta0이, 2번째 원소에서 beta1이 나온다는 것이다. 정규 방정식을 통한 풀이의 (2)번식과 비교하면 <strong class="r">beta1 식이 정확하게 일치</strong>함을 볼 수 있다! 여기까지 오느라 정말 수고 많았다, 나 자신!

한 발 더 나가자면, 정규 방정식을 통한 풀이의 (1)번식에 <strong class="r">beta1의 수식을 대입하면 정확하게 선형대수학을 이용한 풀이의 (2)번식의 beta0과 같아짐</strong>을 확인할 수 있다.

<div class="notice--success">
<h4>정리 2: 선형대수학을 이용한 SLR에서의 LSE는 다음과 같이 구해진다</h4>
<ul>
<li>$$\begin{align}
\hat\beta=
\begin{bmatrix} 
   \frac{\sum x_i^2\bar{y} - \bar{x}\sum x_iy_i}{\sum x_i^2-n\bar{x}^2} \\
   \frac{\sum x_iy_i - n\bar{x}\bar{y}}{\sum x_i^2-n\bar{x}^2} \\
   \end{bmatrix}
=\begin{bmatrix} 
   \hat\beta_0 \\
   \hat\beta_1 \\
   \end{bmatrix}
\end{align}$$</li>
</ul></div>


#### SLR에서의 LSE: R 프로그래밍을 통한 풀이

```R
# lm을 통한 풀이
X1=rnorm(100)
error=rnorm(100)
Y=2 + 3*X1 + error

lm(Y ~ X)

# 선형대수학을 이용한 풀이
X=cbind(1, X1)

solve(t(X) %*% X) %*% (t(X) %*% Y)
```

solve는 역행렬을 계산하는 함수이고 %*%는 행렬곱의 기호이다.

다음 코드에서 Call로 Intercept와 X1이 나오는데, 각각 beta0과 beta1이다. 이들은 parameter(모수)가 아니라 estimator(추정량), 즉 정확히는 $$\hat\beta_0, \hat\beta_1$$임에 유의하자.

![R_SLR]({{site.url}}/images/RegAna/R_SLR.png)

둘 다 추정량에 불과하기에, 모수인 2와 3과 비슷한 1.854, 2.942가 나오긴 하지만 결코 같은 값은 아니다. 그래도 표본으로 모집단을 괜찮게 추론한 듯 하다!

### MLR에서의 LSE 찾기

MLR에서도 회귀계수가 beta0부터 beta p까지 총 (p+1)개일 때, 방정식도 (p+1)개가 나오므로 여전히 solvable한 Normal Equation이 나온다.

여기부터는 회귀계수의 개수와 상관없이 풀이가 도출되어야하므로, 정규방정식은 배제하고 선형대수학적 풀이와 R 코드, 2개만 설명한다.

#### MLR에서의 LSE: 선형대수학을 이용한 풀이

$$y=\beta_0+\beta_1x+\epsilon$$의 식에서 beta0과 beta1을 벡터 beta로 축약하여 SSE를 선형대수학적으로 표현해보자. 첫번째 epsilon은 scalar이고 두번째 epsilon은 각각 벡터이다.

$$\begin{align} SSE&=\sum\epsilon_i^2=\epsilon^T\epsilon=(y-X\beta)^T(y-X\beta)\\&=y^Ty-\beta^TX^Ty-y^TX\beta+\beta^TX^TX\beta\\&:=Q(\beta) \end{align}$$

여기서 우리의 목표는 Q(beta)가 최소가 되게 만드는 것이므로

$$\text{minimize }Q(\beta)\Leftrightarrow \frac{\partial Q(\beta)}{\partial \beta}$$

이때 $$y^Ty$$는 스칼라이고 $$\beta^TX^Ty$$와 $$y^TX\beta$$는 값까지 동일한 스칼라임에 주목하자(스칼라의 transpose는 여전히 똑같은 값의 스칼라이다). 따라서 Q(beta)를 정리하면

$$Q(\beta)=y^Ty-2y^TX\beta+\beta^TX^TX\beta$$

이고 위 식을 베타에 대해 편미분해야한다. 우리는 대칭행렬(symmetric matrix) A에 대해 다음과 같은 미분법을 알고 있다.

$$A:=\text{symmetric}\to \frac{\partial}{\partial \beta}(\beta^TA\beta)=(A+A^T)\beta=2A\beta$$

이때 X transpose와 X의 곱은 전치(transpose)를 취해도 똑같은 행렬이므로, 이 역시 대칭행렬이다. 따라서

$$\frac{\partial Q(\beta)}{\partial \beta}=-2y^TX+2\beta^TX^TX=0$$

이고

$$y^TX=\beta^TX^TX\longrightarrow (X^TX)\beta=X^Ty$$

에서 좌변에 beta만을 남기기 위해 양변에 $$(X^TX)^{-1}$$을 곱하면

<strong class="r">$$(LSE)\;\hat\beta=(X^TX)^{-1}(X^Ty)$$</strong>

LSE인 beta hat을 구할 수 있다. 처음에 beta hat은 벡터라고 했으므로, 첫 번째 원소는 intercept인 beta0, 두 번째 원소 부터는 X_i 각각의 slope인 beta_i이다!

<div class="notice--success">
<h4>정리 3: 선형대수학을 이용한 MLR에서의 LSE는 다음과 같이 구해진다</h4>
<ul>
<li>$$(LSE)\;\hat\beta=(X^TX)^{-1}(X^Ty)$$</li>
</ul></div>

#### MLR에서의 LSE: R 프로그래밍을 통한 풀이(아직 안 배움)

```R
# ~~~ 채워보자 ~~~
```

### Parameters v.s. Estimates

말 그대로 beta와 beta hat은 어떻게 다르냐에 대한 것이다. 표로 정리해보자.

<table border="1">
    <th style="text-align:center">Parameter $$\beta$$</th>
    <th style="text-align:center">Estimator $$\hat\beta$$</th>
	<tr><!-- 첫번째 줄 시작 -->
        <td>Population: true model</td>
	    <td>Sample: estimated model</td>
	</tr><!-- 첫번째 줄 끝 -->
	<tr><!-- 두번째 줄 시작 -->
        <td>$$y=\beta_0+\beta_1x$$</td>
	    <td>$$y=\hat\beta_0+\hat\beta_1x$$</td>
	</tr><!-- 두번째 줄 끝 -->
    <tr><!-- 세번째 줄 시작 -->
        <td>fixed</td>
	    <td>ramdom. changes slightly</td>
	</tr><!-- 세번째 줄 끝 -->
    <tr><!-- 네번째 줄 시작 -->
        <td>of interest</td>
	    <td>not of interest</td>
	</tr><!-- 네번째 줄 끝 -->
    <tr><!-- 다섯번째 줄 시작 -->
        <td>unknown</td>
	    <td>can be calculated</td>
	</tr><!-- 다섯번째 줄 끝 -->
</table>

별개로 추정량과 같은 statistics(통계량)은 모수에 가까울 것이라고 여기는 확률적 값(random number)이다. $$\bar{x}, \hat\mu, \hat\beta_0$$등이 있다.

## 평균 제곱 오차(MSE: Mean Squared Error)

이제 회귀계수에 대해 다뤘으니, 오차(혹은 잔차)에 대해 알아보자.

### 잔차(Residual)의 성질

우리가 관측한 y값과 달리, 회귀선 상에 위치하는 예측값(predicted value) y hat는 <strong class="r">fitted value</strong>이다. 같은 맥락으로 오차(error) epsilon_i는 모수인 회귀계수의 영향을 받으므로 정확히 알 수는 없지만, <strong class="r">관측된 에러인 잔차(residual)</strong>는 정확히 알 수 있다. 따라서 이제는 회귀계수 대신 추정량을 쓰듯, 오차 대신 잔차로 계산하겠다!

앞서 정규 방정식에서 나온 두 식을 잔차로 해석하면

$$\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_{i1}-\cdots)=\sum_{i=1}^ne_i=0$$

$$\sum_{i=1}^nx_{ik}(y_i-\hat\beta_0-\hat\beta_1x_{i1}-\cdots)=\sum_{i=1}^nx_{ik}e_i=0$$

에서 <strong class="r">잔차의 합은 0이고, 잔차와 x는 orthogonal</strong>함을 확인할 수 있다. 다시 강조하지만 <strong class="r">$$x\perp\!\!\!\!\perp e$$</strong>이다.

여기서 X와 e의 상관관계가 무상관임도 확인할 수 있는데, 위 두 식을 잘 적용하면

$$Cov(X_k,e)=\frac{1}{n-1}(\sum_{i=1}^nx_{ik}e_i-n\bar{x}_k\bar{e})=0$$

다음과 같이 X와 e의 공분산이 0임을 알 수 있다. (공분산이 0이므로, 공분산을 통해 계산하는 상관계수 또한 0이 나온다.) 둘은 서로 independent하다!

### MSE 계산

이제 <strong class="r">평균제곱오차(MSE: Mean Square Error)</strong>에 대해 알아보자. MSE는 잔차들의 제곱합을 다 더하여 평균을 낸 수치이다. 그런데 불편추정량(unbiased estimator)을 얻기 위해서는 자유도인 $$n-p-1$$로 평균을 내야한다. 다중선형회귀(MLR)에서 자유도를 계산할 때, 회귀계수 beta0~beta p까지 총 (p+1)개의 회귀계수들을 이미 알고 있기에 그만큼 빼줘야 하기 때문이다.

$$MSE=\frac{\sum_{i=1}^ne_i^2}{n-p-1}=\frac{\sum_{i=1}^n(y_i-\hat y_i)^2}{n-p-1}$$

따라서 다음과 같이 unbiased한 MSE를 구할 수 있다. 왜 불편 추정량이 필요한 지에 대해서는 다음 수업에...


출처: 회귀분석(STAT342) ㅊㅅㅂ 교수님
