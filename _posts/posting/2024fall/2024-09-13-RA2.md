---
layout: single
title: "[회귀분석 2주차] 선형회귀와 최소자승법"
categories: Statistics
tag: [STAT, STAT342]
toc: true # table of contents
author_profile: false # 각 콘텐츠에서 프로필 여부
sidebar: # 사이드바 설정
    nav: "docs"
search: true # 검색 여부 설정
---
<head>
    <!-- Latex -->
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<style>
    th, td {
        text-align: center;
    }
    .r {
        color: red;
    }
    .b {
        color: blue;
    }
</style>

## SLR(단순선형회귀)와 MLR(다중선형회귀)

### 뭐가 선형 회귀일까?

지난 장에서 기본적으로 회귀는 예측 변수 X에서 응답 변수 Y에 대한 함수적 관계 f를 찾는 것이라고 했다. 즉

$$Y=f(X_1,X_2,\cdots,X_p)$$

이고 함수 f는 흔히, 알지 못하는 상태로 주어진다. 이제 함수적 관계 f를 찾아가는 과정을 배우겠다.

여기서 X1부터 Xp까지의 data는 deterministic한 함수 f의 신호(signal)로 입력된다. 그리고 나머지 불확실성은 noise인 error(epsilon)이 먹게 된다. 따라서 이 신호를 얼마나 잘 살리느냐가 핵심인데, 이 신호를 살리기 가장 간단한 형태가 <strong class="r">선형 회귀</strong>이다.

그런데 넓은 의미에서 '선형성'이라는 것은 변수 X가 아닌 <strong class="r">회귀 계수와 종속 변수간의 선형성</strong>을 의미한다. 이게 무슨 의미냐고? 아래의 수식들이 사실 linear regression 모델이라는 뜻이다.

$$Y=\beta_0+\beta_1X+\beta_2$$<strong class="b">$$X^2$$</strong>$$+\epsilon$$

$$Y=\beta_0+\beta_1$$<strong class="b">$$log(X)$$</strong>$$+\epsilon$$

누가 봐도 비선형 모델 아닌가? 라고 생각했었다. 아니라 한다. 왜냐? 첫번째 식의 경우 변수 X^2을 그냥 X2라는 새로운 변수로 생각하면, 이 식은 다중 선형 회귀가 된다. 로그 X도 새로운 변수 X3로 치환해버리면 이 역시 선형성을 띠게 된다. 하나 못해 아래 식을 보라

### 비선형 모델을 선형 모델로 바꾸기

$$y=X^\beta\epsilon\text{ 에서 양변에 로그를 취하면}$$

$$log\;y=\beta log\;X+log\;\epsilon\text{ 이므로 }y'=\beta X'+\epsilon'$$

어라? 비선형 모델이 선형 모델로 뚝딱 바뀌었다.

$$Y=\frac{X}{\alpha X+\beta}$$

얘도 가능할까? 역수를 취한뒤 치환을 하면

$$\frac{1}{Y}=\alpha+\beta(\frac{1}{X})\text{ 에서 }Y'=\alpha+\beta X'$$

감쪽같다. 이번엔 경제학 분야에서 쓰이는 Cobb-Douglas production function을 살펴보자. 노동(L)과 자본(K)이 생산량(Y)을 결정한다는 의미의 함수이다.

$$V=\alpha K^{\beta_1}L^{\beta_2}\text{ 에서 양변에 로그를 취하면}$$

$$log(V)=log(\alpha)+\beta_1log(K)+\beta_2 log(L)$$

익히 봐서 알겠지만 로그꼴의 변수들을 새로운 변수로 정의하면 선형이 되어버린다. 자 이제 시사하는 바는 <strong class="r">선형 회귀는 강력하다</strong>는 것이다. 비선형 모델을 선형적으로 해석이 가능하고, 선형 회귀는 다루기 비교적 쉬우니, 우리는 선형 회귀부터 열심히 공부하면 된다!

다만 $$Y=\beta_0\beta_1^X+\epsilon$$처럼 회귀계수끼리의 곱은 선형 모델로 만들 수 없다. 로그를 취해봐도 안 됨을 알 수 있다. 아마 선형성이 '회귀계수와 종속변수'간의 것이라, 회귀계수끼리 곱해진 건 안되는 듯

### 다중 선형 회귀 모델(MLRM)로의 확장

![SMLR]({{site.url}}/images/RegAna/SLR_MLR.png)

회귀에서 각 row(혹은 pairs)는 case이다.(subject(피험체)이거나 individual(개개인)) record나 data point라고도 부른다. SLR(단순 선형 회귀)는 single input에 single response인 반면, MLR(다중 선형 회귀)는 multiple inputs이라는 차이가 있다.

$$\textbf{MLRM: }\;y_i=\beta_0+\beta_1x_{i1}+\cdots+\beta_px_{ip}+\epsilon_i\gets \text{SLRM if p=1}$$

MLRM에서 p=1이면 SLRM인 것처럼, MLRM은 SLRM의 확장이다. 엡실론이 정규분포 N(0,sigma^2)에서 iid하게 뽑혔다고 가정해보자. 즉 error의 분산은 시그마 제곱인 것이다.

여기서 절편(intercept) beta0이나 회귀계수(기울기) beta_k, 에러의 분산 sigma^2 등은 모수(parameters)이다. 원래부터 정해져있는, 랜덤 값이 아닌 상수라는 것. 신기한 점은 관측값 X도 모수라는 것이다.

그러나 에러 epsilon이나 출력 Y는 확률 변수(random variable, a.k.a 'r.v.')라는 점에 유의하자. 우리는 관측된 입력 변수 X, 출력 변수 Y를 통하여서, 알려지지 않은 회귀 계수들과 에러 및 에러의 분산 등을 찾아내야한다.

![MLRM]({{site.url}}/images/RegAna/MLRM_matrix.png)

벡터 X, Y, epsilon과 행렬 X로 MLRM(다중 선형 회귀 모델)을 표현하면 다음과 같다. n은 관측 값의 개수, p는 입력 하나 당의 특성치(features)이다.

중간의 n*(p+1)짜리 행렬 X를 <strong class="r">design matrix</strong>라 하는데, 여기서 1열의 1은 y절편과 곱해지는 곱셈의 항등원이고, 그 오른쪽은 변수 X에 대한 값들이다. 이 design matrix는 바로 옆의 벡터 beta와 곱해지는데, beta0은 절편을 결정하고, 나머지는 회귀계수들이 들어간다.

## 최소자승추정법(Least Squared Estimation)

이제 베타(회귀계수)값들을 찾아보자. 이제부터 사용할 방법은 LS라고 불리는 <strong class="r">Least Squares Method(최소자승법)</strong>이다. 뭐 철자가 조금씩 바뀌긴 하지만 통칭 LS라고 불린다.

![line]({{site.url}}/images/RegAna/regression_line.png)

여기서 빨간 회귀선은 $$y=\beta_0+\beta_1x+\epsilon$$의 식을 따른다. 이때 각 점들에 해당하는 y값들이 회귀선 위에 있다면 에러(error)가 양수, 아래에 있다면 에러가 음수이다. 여기서 핵심은, 각 점들로부터의 에러가 최소가 되는 회귀선을 찾는 것이다. 그렇다면 모든 에러의 합이 최소가 되도록 하는 회귀선을 찾으면 될까?

그렇지 않다. 적절한 회귀선을 찾았다면 <strong class="r">에러의 단순합은 반드시 0</strong>이 된다. 즉 $$\sum_i\epsilon_i=0$$인 것으로, 단순합으로는 아무런 정보를 얻을 수가 없다는 사실을 알 수 있다. 따라서 우리는 제곱합을 사용하는데, 
이를<strong class="r">SSE</strong>(Sum of Squared Error)라고 부른다. 따라서 SSE는

$$SSE=\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)^2=\sum_{i=1}\epsilon_i^2$$

다음과 같이 각 포인트 y에서 회귀선의 fitted value인 y hat를 뺀 값들의 제곱합임을 알 수 있다. 그렇다면 SSE가 최소가 되게 하는 회귀계수를 찾아야하는데, 제곱의 최소를 찾는다? 답이 보이는가? 각 회귀계수에 대하여 SSE를 <strong class="r">편미분</strong>하면 된다! 이렇게 편미분을 통해 나온 회귀계수 <strong class="r">$$\hat\beta$$가 LSE</strong>(Least Squared Estimator)다. 추정량이기에 hat 기호로 표기함을 확인하자.

### SLR에서의 LSE 찾기

이제 LSE가 어떤 값이 나와서 회귀선이 어떻게 그려지는 지를 알아볼 것이다. 앞으로는 (1) 정규방정식을 통한 풀이 / (2) 선형대수학을 이용한 풀이 / (3) R 프로그래밍을 통한 풀이, 이렇게 3가지로 나눠서 각기 설명하겠다. (1)번과 (2)번은 서로 연관돼있어서 경계를 넘나들므로 두 풀이의 관계를 잘 살피며 이해해보자.

#### SLR에서의 LSE: 정규방정식을 통한 풀이

앞서 LSE를 다음과 같이 표기한다고 다뤘다.

$$L(\hat\beta_0,\hat\beta_1)=\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)^2$$

따라서 LSE에 대한 함수 L을 beta0과 beta1로 각각 편미분한 편미분방정식은

$$\frac{\partial L}{\partial \hat\beta_0}=-2\sum_{i=1}^n(y_i-\hat\beta_0-\hat\beta_1x_i)=0$$

$$\frac{\partial L}{\partial \hat\beta_1}=-2\sum_{i=1}^nx_i(y_i-\hat\beta_0-\hat\beta_1x_i)=0$$

여기서 알 수 있는 점은 (1) <strong class="r">잔차합(에러의 단순합)은 항상 0</strong>이라는 것과, <strong class="r">(x_i * 잔차)의 합도 항상 0</strong>이라는 것이다. 이처럼 편미분 방정식이 이변수이고, 식도 2개가 나와서 답이 나오는(solvable) 방정식을 <strong class="r">정규 방정식(Normal Equation)</strong>이라고 한다. 잘 기억해두자.

이 정규 방정식을 의미론적으로 해석하면

$$\sum e_i=\sum x_ie_i=0\Rightarrow$$ <strong class="r">$$x\perp\!\!\!\!\perp e$$</strong>

즉, signal X와 에러 e는 통계적으로 독립이라는 뜻이다. signal인 X에 의해 회귀계수인 beta들이 결정되고, 나머지 필요없는 noise들이 error에 들어간다는 종전의 이론에 정확히 맞아 떨어진다!!

이야기가 딴 데로 새버렸는데, 아무튼 이 두 식을 정리하면

$$n\hat\beta_0+\hat\beta_1\sum_{i=1}^nx_i=\sum_{i=1}^ny_i$$

$$\hat\beta_0\sum_{i=1}^nx_i+\hat\beta_1\sum_{i=1}^nx_i^2=\sum_{i=1}^nx_iy_i$$

이 나온다. 그런데 $$\sum_{i=1}^nx_i=n\bar{x},\sum_{i=1}^ny_i=n\bar{y}$$이므로 대입하여 식을 변환시키면

<strong class="r">$$\text{(1) }\hat\beta_0=\bar{y}-\hat\beta_1\bar{x}$$</strong>

$$\hat\beta_0n\bar{x}+\hat\beta_1\sum_{i=1}^nx_i^2=\sum_{i=1}^nx_iy_i$$

이다. (1)번식에서 beta 1 hat을 구하면 beta 0 hat이 구해짐을 알 수 있다. 이제 (1)번식을 아래 식에 대입하면

$$(\bar{y}-\hat\beta_1\bar{x})n\bar{x}+\hat\beta_1\sum_{i=1}^nx_i^2=\sum_{i=1}^nx_iy_i$$

$$\hat\beta_1(\sum_{i=1}^nx_i^2-n\bar{x}^2)=\sum_{i=1}^nx_iy_i-n\bar{x}\bar{y}$$

<strong class="r">$$\text{(2) }\hat\beta_1=\frac{\sum_{i=1}^nx_iy_i-n\bar{x}\bar{y}}{\sum_{i=1}^nx_i^2-n\bar{x}^2}:=\frac{A}{B}$$</strong>

이 도출된다. (2)번식을 잘 기억하자. 이제 2번식의 분자를 A, 분모를 B라 두고 계속 변환해 보겠다.

$$\sum(x_i-\bar{x})=0\longrightarrow \sum(x_i-\bar{x})\bar{y}=0$$

그 전에 다음 식을 하나 보자. i에 대해 이미 합이 0인 식이기에, 상수 y bar를 곱해도 똑같이 0이라는 사실에 주목하자. centered된 x와 y에 대하여 위 식을 사용하면 분자의 식 A는

$$\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})=\sum_{i=1}^n(x_i-\bar{x})y_i=\sum_{i=1}^nx_iy_i-n\bar{x}\bar{y}:=A$$

또한 아래 분모의 식 B는

$$\sum_{i=1}^n(x_i-\bar{x})^2=\sum_{i=1}^nx_i^2-n\bar{x}^2:=B$$

이므로 두 식 A와 B를 (2)번식에 대입하면

$$\hat\beta_1=\frac{\sum_{i=1}^nx_iy_i-n\bar{x}\bar{y}}{\sum_{i=1}^nx_i^2-n\bar{x}^2}=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}=\frac{\sum_{i=1}^n(x_i-\bar{x})y_i}{\sum_{i=1}^n(x_i-\bar{x})^2}$$

이고 특히 가운데 식에 주목하면

<strong class="r">$$\text{(3) }\hat\beta_1=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}=\frac{Cov(X,Y)}{Var(X)}$$</strong>

3번 식이 나온다! 이를 통해 slope(기울기)인 <strong class="r">beta1은 X의 분산에 대한 공분산 X, Y의 비율</strong>임을 알 수 있다!

<div class="notice--warning">
<h4>정리 1: 정규방정식을 통한 SLR에서의 LSE는 다음과 같이 구해진다</h4>
<ul>
<li>$$\hat\beta_0=\bar{y}-\hat\beta_1\bar{x}$$</li>
<li>$$\hat\beta_1=\frac{\sum_{i=1}^n(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n(x_i-\bar{x})^2}=\frac{Cov(X,Y)}{Var(X)}$$</li>
</ul></div>

#### SLR에서의 LSE: 선형대수학을 이용한 풀이

이 풀이는 후술할 'MLR에서의 LSE: 선형대수학을 이용한 풀이'에서 이어지는 내용이다. 아직 MLR 파트를 읽지 않았다면 아래로 내려가서 그 파트를 먼저 읽고, 다시 돌아오자.



#### SLR에서의 LSE: R 프로그래밍을 통한 풀이

```R
# lm을 통한 풀이
X1=rnorm(100)
error=rnorm(100)
Y=2 + 3*X1 + error

lm(Y ~ X)

# 선형대수학을 이용한 풀이
X=cbind(1, X1)

solve(t(X) %*% X) %*% (t(X) %*% Y)
```

solve는 역행렬을 계산하는 함수이고 %*%는 행렬곱의 기호이다.

다음 코드에서 Call로 Intercept와 X1이 나오는데, 각각 beta0과 beta1이다. 이들은 parameter(모수)가 아니라 estimator(추정량), 즉 정확히는 $$\hat\beta_0, \hat\beta_1$$임에 유의하자.

둘 다 추정량에 불과하기에, 모수인 2와 3과 비슷한 1.854, 2.942가 나오긴 하지만 결코 같은 값은 아니다.

![R_SLR]({{site.url}}/images/RegAna/R_SLR.png)

### MLR에서 LSE 찾기

마찬가지 방식으로 세 개로 나누어 설명한다.

#### MLR에서 LSE 찾기: 정규방정식을 통한 풀이




