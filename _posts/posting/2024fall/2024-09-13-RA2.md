---
layout: single
title: "[회귀분석 2주차] 선형회귀와 최소자승법"
categories: Statistics
tag: [STAT, STAT342]
toc: true # table of contents
author_profile: false # 각 콘텐츠에서 프로필 여부
sidebar: # 사이드바 설정
    nav: "docs"
search: true # 검색 여부 설정
---
<head>
    <!-- Latex -->
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
</head>
<style>
    th, td {
        text-align: center;
    }
    .r {
        color: red;
    }
    .b {
        color: blue;
    }
</style>

## SLR(단순선형회귀)와 MLR(다중선형회귀)

### 뭐가 선형 회귀일까?

지난 장에서 기본적으로 회귀는 예측 변수 X에서 응답 변수 Y에 대한 함수적 관계 f를 찾는 것이라고 했다. 즉

$$Y=f(X_1,X_2,\cdots,X_p)$$

이고 함수 f는 흔히, 알지 못하는 상태로 주어진다. 이제 함수적 관계 f를 찾아가는 과정을 배우겠다.

여기서 X1부터 Xp까지의 data는 deterministic한 함수 f의 신호(signal)로 입력된다. 그리고 나머지 불확실성은 noise인 error(epsilon)이 먹게 된다. 따라서 이 신호를 얼마나 잘 살리느냐가 핵심인데, 이 신호를 살리기 가장 간단한 형태가 <strong class="r">선형 회귀</strong>이다.

그런데 넓은 의미에서 '선형성'이라는 것은 변수 X가 아닌 <strong class="r">회귀 계수와 종속 변수간의 선형성</strong>을 의미한다. 이게 무슨 의미냐고? 아래의 수식들이 사실 linear regression 모델이라는 뜻이다.

$$Y=\beta_0+\beta_1X+\beta_2$$$$<strong class="b">X^2</strong>+\epsilon$$

$$Y=\beta_0+\beta_1$$$$<strong class="b">log(X)</strong>+\epsilon$$

누가 봐도 비선형 모델 아닌가? 라고 생각했었다. 아니라 한다. 왜냐? 첫번째 식의 경우 변수 X^2을 그냥 X2라는 새로운 변수로 생각하면, 이 식은 다중 선형 회귀가 된다. 로그 X도 새로운 변수 X3로 치환해버리면 이 역시 선형성을 띠게 된다. 하나 못해 아래 식을 보라

### 비선형 모델을 선형 모델로 바꾸기

$$y=X^\beta\epsilon\text{ 에서 양변에 로그를 취하면}$$

$$log\;y=\beta log\;X+log\;\epsilon\text{ 이므로 }y'=\beta X'+\epsilon'$$

와 거짓말 치지 말자. 비선형 모델이 선형 모델로 뚝딱 바뀌었다.

$$Y=\frac{X}{\alpha X+\beta}$$

얘도 가능할까? 역수를 취한뒤 치환을 하면

$$\frac{1}{Y}=\alpha+\beta(\frac{1}{X})\text{ 에서 }Y'=\alpha+\beta X'$$

감쪽같다. 이번엔 경제학 분야에서 쓰이는 Cobb-Douglas production function을 살펴보자. 노동(L)과 자본(K)이 생산량(Y)을 결정한다는 의미의 함수이다.

$$V=\alpha K^{\beta_1}L^{\beta_2}\text{ 에서 양변에 로그를 취하면}$$

$$log(V)=log(\alpha)+\beta_1log(K)+\beta_2 log(L)$$

익히 봐서 알겠지만 로그꼴의 변수들을 새로운 변수로 정의하면 선형이 되어버린다. 자 이제 시사하는 바는 <strong class="r">선형 회귀는 강력하다</strong>는 것이다. 비선형 모델을 선형적으로 해석이 가능하고, 선형 회귀는 다루기 비교적 쉬우니, 우리는 선형 회귀부터 열심히 공부하면 된다!

다만 $$Y=\beta_0\beta_1^X+\epsilon$$처럼 회귀계수끼리의 곱은 선형 모델로 만들 수 없다. 로그를 취해봐도 안 됨을 알 수 있다. 아마 선형성이 '회귀계수와 종속변수'간의 것이라, 회귀계수끼리 곱해진 건 안되는 듯

### 다중 선형 회귀 모델(MLRM)로의 확장

![SMLR]({{site.url}}/images/RegAna/SLR_MLR.png)

회귀에서 각 row(혹은 pairs)는 case이다.(subject(피험체)이거나 individual(개개인)) record나 data point라고도 부른다. SLR(단순 선형 회귀)는 single input에 single response인 반면, MLR(다중 선형 회귀)는 multiple inputs이라는 차이가 있다.

$$\textbf{MLRM: }\;y_i=\beta_0+\beta_1x_{i1}+\cdots+\beta_px_{ip}+\epsilon_i\gets \text{SLRM if p=1}$$

MLRM에서 p=1이면 SLRM인 것처럼, MLRM은 SLRM의 확장이다. 엡실론이 정규분포 N(0,sigma^2)에서 iid하게 뽑혔다고 가정해보자. 즉 error의 분산은 시그마 제곱인 것이다.

여기서 절편(intercept) beta0이나 회귀계수(기울기) beta_k, 에러의 분산 sigma^2 등은 모수(parameters)이다. 원래부터 정해져있는, 랜덤 값이 아닌 상수라는 것. 신기한 점은 관측값 X도 모수라는 것이다.

그러나 에러 epsilon이나 출력 Y는 확률 변수(random variable, a.k.a 'r.v.')라는 점에 유의하자. 우리는 관측된 입력 변수 X, 출력 변수 Y를 통하여서, 알려지지 않은 회귀 계수들과 에러 및 에러의 분산 등을 찾아내야한다.

![MLRM]({{site.url}}/images/RegAna/MLRM_matrix.png)

벡터 X, Y, epsilon과 행렬 X로 MLRM(다중 선형 회귀 모델)을 표현하면 다음과 같다. n은 관측 값의 개수, p는 입력 하나 당의 특성치(features)이다.

중간의 n*(p+1)짜리 행렬 X를 <strong class="r">design matrix</strong>라 하는데, 여기서 1열의 1은 y절편과 곱해지는 곱셈의 항등원이고, 그 오른쪽은 변수 X에 대한 값들이다. 이 design matrix는 바로 옆의 벡터 beta와 곱해지는데, beta0은 절편을 결정하고, 나머지는 회귀계수들이 들어간다.

## 최소자승추정법(Least Squared Estimation)






